import React, { useState, useEffect, useRef } from 'react';
import { Bot, X, Send, Volume2, VolumeX, Sparkles, Crown, Star, Settings, Home, Gamepad2, Users, FileText, User, BarChart3, Info, Brain, Zap, Target, TrendingUp, Lightbulb, RotateCcw, Play, Pause, HelpCircle, Award, Clock, Activity, Maximize2, Minimize2, MessageSquare, History, Mic, MicOff, Bell, Globe, Shield, Calculator, BookOpen, Palette, Music, Heart } from 'lucide-react';
import { useNavigate, useLocation } from 'react-router-dom';
import { useTheme } from '../contexts/ThemeContext';
import { AblyConversationService } from '../services/AblyConversationService';
import { getAgentId, getVoiceId } from '../config/elevenlabs';
import { useAIAgent } from '../contexts/AIAgentContext';
import { useUser } from '../contexts/UserContext';
import { base64ToBinary } from '../utils/videoUtils';
import { elevenLabsService } from '../services/elevenLabsService';
import { WebSocketService } from '../services/WebSocketService';

import PlasmaBall from './PlasmaBall';
import './AIAssistant.css';

interface Message {
    text: string;
    isUser: boolean;
    timestamp: Date;
    image?: string;
    action?: 'navigation' | 'settings' | 'info' | 'agent' | 'game';
}

interface Conversation {
    id: string;
    title: string;
    messages: Message[];
    timestamp: Date;
    context?: string;
}

// Define the assistant modes for cleaner state management
type AssistantMode = 'idle' | 'text_input' | 'single_recording' | 'conversing';

// Language selection states and constants
type LanguageCode = 'auto' | 'en' | 'en_us' | 'en_uk' | 'es' | 'fr' | 'de' | 'id' | 'it' | 'ja' | 'nl' | 'pl' | 'pt' | 'ru' | 'tr' | 'uk' | 'ca' | 'ar' | 'az' | 'bg' | 'bs' | 'zh' | 'cs' | 'da' | 'el' | 'et' | 'fi' | 'fil' | 'gl' | 'hi' | 'hr' | 'hu' | 'ko' | 'mk' | 'ms' | 'nb' | 'ro' | 'sk' | 'sv' | 'th' | 'ur' | 'vi' | 'yue';
const SUPPORTED_LANGUAGES: Record<LanguageCode, string> = {
    'auto': 'ðŸŒ Auto-Detect',
    // High accuracy languages (â‰¤ 10% WER)
    'en': 'English',
    'en_us': 'English (US)',
    'en_uk': 'English (UK)',
    'es': 'Spanish',
    'fr': 'French',
    'de': 'German',
    'id': 'Indonesian',
    'it': 'Italian',
    'ja': 'Japanese',
    'nl': 'Dutch',
    'pl': 'Polish',
    'pt': 'Portuguese',
    'ru': 'Russian',
    'tr': 'Turkish',
    'uk': 'Ukrainian',
    'ca': 'Catalan',
    // Good accuracy languages (>10% to â‰¤25% WER)
    'ar': 'Arabic',
    'az': 'Azerbaijani',
    'bg': 'Bulgarian',
    'bs': 'Bosnian',
    'zh': 'Chinese (Mandarin)',
    'cs': 'Czech',
    'da': 'Danish',
    'el': 'Greek',
    'et': 'Estonian',
    'fi': 'Finnish',
    'fil': 'Filipino',
    'gl': 'Galician',
    'hi': 'Hindi',
    'hr': 'Croatian',
    'hu': 'Hungarian',
    'ko': 'Korean',
    'mk': 'Macedonian',
    'ms': 'Malay',
    'nb': 'Norwegian',
    'ro': 'Romanian',
    'sk': 'Slovak',
    'sv': 'Swedish',
    'th': 'Thai',
    'ur': 'Urdu',
    'vi': 'Vietnamese',
    'yue': 'Cantonese'
};

const AIAssistant: React.FC = () => {
    const [isOpen, setIsOpen] = useState(false);
    const [isFullscreen, setIsFullscreen] = useState(false);
    const [activeConversation, setActiveConversation] = useState<string>('current');
    const [conversations, setConversations] = useState<{ [key: string]: Conversation }>({
        current: {
            id: 'current',
            title: 'Current Chat',
            messages: [
                { 
                    text: "ðŸŒŸ Hi there! I'm Synapse, your friendly AI teacher! ðŸš€ I can help you with learning, games, navigation, settings, AI agent control, and accessibility needs. What would you like to explore today? ðŸŽ®âœ¨", 
                    isUser: false, 
                    timestamp: new Date() 
                }
            ],
            timestamp: new Date()
        }
    });
    
    const menuRef = useRef<HTMLDivElement>(null);
    const [inputMessage, setInputMessage] = useState('');
    const [isLoading, setIsLoading] = useState(false);
    const [isPlayingVoice, setIsPlayingVoice] = useState(false);
    const [promptCount, setPromptCount] = useState(0);
    const [isReadingLastMessage, setIsReadingLastMessage] = useState(false);
    
    // Platform-specific conversation services
    const conversationService = useRef<AblyConversationService>(AblyConversationService.getInstance());
    const webSocketService = useRef<WebSocketService>(WebSocketService.getInstance());
    
    // NEW: Single mode state instead of multiple booleans
    const [mode, setMode] = useState<AssistantMode>('idle');
    const modeRef = useRef<AssistantMode>('idle'); // Add ref to track current mode
    
    // Voice recording states (simplified)
    const [isPlasmaPressed, setIsPlasmaPressed] = useState(false);
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const streamingTranscriptionRef = useRef<any>(null);

    // Chunked voice generation system
    const [voiceChunks, setVoiceChunks] = useState<string[]>([]);
    const [isProcessingVoice, setIsProcessingVoice] = useState(false);
    const voiceChunkTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);

    // Language selection states and constants
    const [selectedLanguage, setSelectedLanguage] = useState<LanguageCode>('auto'); // Default to auto-detect
    const [showLanguageDropdown, setShowLanguageDropdown] = useState(false);

    // Language selection handlers
    const toggleLanguageDropdown = () => setShowLanguageDropdown(prev => !prev);
    const handleLanguageSelect = (code: LanguageCode) => {
        setSelectedLanguage(code);
        setShowLanguageDropdown(false);
    };
    
    const navigate = useNavigate();
    const location = useLocation();
    const theme = useTheme();
    const { triggerAgent, showAgent, hideAgent, currentContext, updateContext } = useAIAgent();
    const { user } = useUser();
    
    // Remove limits for DemoUser
    const isDemoUser = user?.username === 'DemoUser';

    // Debug MediaRecorder on mount
    useEffect(() => {
        debugMediaRecorder();
    }, []);

    // Get current messages based on active conversation
    const currentMessages = conversations[activeConversation]?.messages || [];

    // Available pages and their descriptions
    const availablePages = {
        '/': { name: 'Home', icon: Home, description: 'Main landing page' },
        '/playground': { name: 'Playground', icon: Gamepad2, description: 'Games and activities' },
        '/dashboard': { name: 'Dashboard', icon: BarChart3, description: 'Your learning progress' },
        '/forum': { name: 'Forum', icon: Users, description: 'Community discussions' },
        '/forum-registration': { name: 'Forum Registration', icon: FileText, description: 'Join the forum' },
        '/registration': { name: 'Registration', icon: User, description: 'Create an account' },
        '/signin': { name: 'Sign In', icon: User, description: 'Login to your account' },
        '/ai-report': { name: 'AI Report', icon: BarChart3, description: 'AI learning analytics' },
        '/about': { name: 'About Us', icon: Info, description: 'Learn about NeuraPlay' },
        '/counting-test': { name: 'Counting Test', icon: Gamepad2, description: 'Math practice' },
        '/test': { name: 'Test Page', icon: Gamepad2, description: 'Testing features' },
        '/text-reveal': { name: 'Text Reveal', icon: Sparkles, description: 'Text animations' },
        '/old-home': { name: 'Old Home', icon: Home, description: 'Previous home page' },
        '/profile': { name: 'Profile', icon: User, description: 'Your user profile' },
        '/user-profile': { name: 'User Profile', icon: User, description: 'Detailed user profile' }
    };

    // Available settings and their descriptions - COMPLETE LIST with agency
    const availableSettings = {
        'theme': { 
            name: 'Theme', 
            icon: Settings, 
            description: 'Change light/dark mode',
            actions: ['toggle_dark_mode', 'set_theme_preference']
        },
        'accessibility': { 
            name: 'Accessibility', 
            icon: Target, 
            description: 'Accessibility settings including color blindness support',
            actions: ['test_color_vision', 'enable_colorblind_mode', 'adjust_contrast', 'increase_font_size']
        },
        'notifications': { 
            name: 'Notifications', 
            icon: Bell, 
            description: 'Notification preferences',
            actions: ['toggle_notifications', 'set_reminder_frequency', 'customize_alerts']
        },
        'language': { 
            name: 'Language', 
            icon: Globe, 
            description: 'Language settings',
            actions: ['change_language', 'set_speech_language', 'enable_translation']
        },
        'privacy': { 
            name: 'Privacy', 
            icon: Shield, 
            description: 'Privacy settings',
            actions: ['manage_data_sharing', 'set_parental_controls', 'review_permissions']
        },
        'voice': { 
            name: 'Voice Settings', 
            icon: Mic, 
            description: 'Voice recognition and TTS preferences',
            actions: ['adjust_voice_speed', 'change_voice_character', 'calibrate_microphone']
        },
        'profile': { 
            name: 'Profile', 
            icon: User, 
            description: 'User profile and learning preferences',
            actions: ['update_learning_goals', 'set_difficulty_level', 'customize_avatar']
        }
    };

    // Environment detection functions
    const isNetlify = () => {
        return window.location.hostname.includes('netlify') ||
               window.location.hostname.includes('localhost') ||
               window.location.hostname.includes('127.0.0.1');
    };

    const isRender = () => {
        return window.location.hostname.includes('onrender');
    };

    // Available games and their descriptions with developmental benefits
    const availableGames = {
        'counting': { 
            name: 'Counting Adventure', 
            icon: Gamepad2, 
            description: 'Learn to count with fun games',
            benefits: 'Develops number recognition, counting skills, and basic math foundations'
        },
        'fuzzling': { 
            name: 'Fuzzling Game', 
            icon: Target, 
            description: 'Interactive shape and pattern matching',
            benefits: 'Improves visual processing, pattern recognition, and cognitive flexibility'
        },
        'memory': { 
            name: 'Memory Sequence', 
            icon: Brain, 
            description: 'Test and improve memory skills',
            benefits: 'Enhances working memory, attention span, and sequential processing'
        },
        'inhibition': { 
            name: 'Inhibition Control', 
            icon: Shield, 
            description: 'Practice self-control and focus',
            benefits: 'Builds impulse control, attention regulation, and executive function'
        },
        'cube': { 
            name: 'The Cube Game', 
            icon: Target, 
            description: 'Spatial reasoning and 3D thinking',
            benefits: 'Develops spatial intelligence, problem-solving, and visual-motor skills'
        },
        'stacker': { 
            name: 'Stacker Challenge', 
            icon: TrendingUp, 
            description: 'Balance and coordination game',
            benefits: 'Improves hand-eye coordination, planning, and motor control'
        },
        'mountain': { 
            name: 'Mountain Climber', 
            icon: TrendingUp, 
            description: 'Adventure and problem-solving',
            benefits: 'Enhances strategic thinking, perseverance, and goal-setting'
        },
        'letter': { 
            name: 'Letter Hunt', 
            icon: BookOpen, 
            description: 'Letter recognition and phonics',
            benefits: 'Builds literacy skills, letter recognition, and reading readiness'
        }
    };

    // Available AI agent personalities
    const availablePersonalities = {
        'synapse-normal': { name: 'Synapse Normal', icon: Brain, description: 'Friendly and helpful AI teacher' },
        'coach': { name: 'Coach', icon: Target, description: 'Motivational and goal-oriented' },
        'mentor': { name: 'Mentor', icon: Lightbulb, description: 'Wise and guiding' },
        'friend': { name: 'Friend', icon: Heart, description: 'Supportive and casual' },
        'analyst': { name: 'Analyst', icon: BarChart3, description: 'Detailed and analytical' }
    };

    // AI Agent commands and their descriptions
    const aiAgentCommands = {
        'show': { name: 'Show AI Agent', description: 'Display the AI agent', keywords: ['show', 'open', 'display', 'bring', 'activate'] },
        'hide': { name: 'Hide AI Agent', description: 'Hide the AI agent', keywords: ['hide', 'close', 'remove', 'dismiss', 'stop'] },
        'trigger': { name: 'Trigger AI Agent', description: 'Activate AI agent with specific trigger', keywords: ['trigger', 'activate', 'start', 'begin', 'launch'] },
        'personality': { name: 'Change AI Personality', description: 'Change AI agent personality', keywords: ['personality', 'style', 'mood', 'character'] },
        'context': { name: 'Update Context', description: 'Update AI agent context', keywords: ['context', 'update', 'set', 'change'] },
        'coach': { name: 'Coach Mode', description: 'Set AI to coach personality', keywords: ['coach', 'teacher', 'instructor'] },
        'mentor': { name: 'Mentor Mode', description: 'Set AI to mentor personality', keywords: ['mentor', 'guide', 'advisor'] },
        'friend': { name: 'Friend Mode', description: 'Set AI to friend personality', keywords: ['friend', 'buddy', 'pal'] },
        'analyst': { name: 'Analyst Mode', description: 'Set AI to analyst personality', keywords: ['analyst', 'analyzer', 'researcher'] }
    };

    // Child-friendly prompt suggestions
    const childPrompts = [
        "Tell me a fun fact about space! ðŸŒŸ",
        "What's the coolest animal you know? ðŸ¦",
        "Can you help me with my homework? ðŸ“š",
        "Tell me a joke! ðŸ˜„",
        "What's your favorite color? ðŸŽ¨",
        "How do plants grow? ðŸŒ±",
        "What makes rainbows? ðŸŒˆ",
        "Tell me about dinosaurs! ðŸ¦•",
        "How do computers work? ðŸ’»",
        "What's the biggest ocean? ðŸŒŠ",
        "Can you teach me to count? ðŸ”¢",
        "What's the weather like? â˜€ï¸",
        "Tell me about the planets! ðŸª",
        "How do birds fly? ðŸ¦…",
        "What's your favorite food? ðŸ•"
    ];

    // Toggle fullscreen mode
    const toggleFullscreen = () => {
        setIsFullscreen(!isFullscreen);
    };

    // Create new conversation
    const createNewConversation = () => {
        const newId = `conv_${Date.now()}`;
        const newConversation: Conversation = {
            id: newId,
            title: `Chat ${Object.keys(conversations).length}`,
            messages: [
                { 
                    text: "ðŸŒŸ Hi there! I'm Synapse, your friendly AI teacher! ðŸš€ What would you like to explore today? ðŸŽ®âœ¨", 
                    isUser: false, 
                    timestamp: new Date() 
                }
            ],
            timestamp: new Date()
        };
        
        setConversations(prev => ({
            ...prev,
            [newId]: newConversation
        }));
        setActiveConversation(newId);
    };

    // Update conversation title
    const updateConversationTitle = (conversationId: string, title: string) => {
        setConversations(prev => ({
            ...prev,
            [conversationId]: {
                ...prev[conversationId],
                title
            }
        }));
    };

    // Add message to current conversation
    const addMessageToConversation = (conversationId: string, message: Message) => {
        setConversations(prev => ({
            ...prev,
            [conversationId]: {
                ...prev[conversationId],
                messages: [...prev[conversationId].messages, message]
            }
        }));
    };

    useEffect(() => {
        // This effect handles closing the menu if clicked outside
        const handlePointerDown = (event: PointerEvent) => {
            if (menuRef.current && !menuRef.current.contains(event.target as Node)) {
                setIsOpen(false);
            }
        };
        document.addEventListener('pointerdown', handlePointerDown);
        return () => document.removeEventListener('pointerdown', handlePointerDown);
    }, []);

    // Update ref whenever mode changes
    useEffect(() => {
        modeRef.current = mode;
    }, [mode]);

    // Platform-specific conversation service event handlers
    useEffect(() => {
        const service = conversationService.current;
        const wsService = webSocketService.current;
        
        // Ably (Netlify) event handlers
        service.on('conversation_ready', (data) => {
            console.log('Conversation ready with ElevenLabs');
            addMessageToConversation(activeConversation, { 
                text: "Voice conversation ready! Start speaking! ðŸŽ¤", 
                isUser: false, 
                timestamp: new Date() 
            });
        });
        
        // WebSocket (Render) event handlers
        wsService.on('connected', () => {
            console.log('âœ… WebSocket connected for conversation');
            if (modeRef.current === 'conversing') {
                addMessageToConversation(activeConversation, { 
                    text: "Voice conversation ready! Start speaking! ðŸŽ¤", 
                    isUser: false, 
                    timestamp: new Date() 
                });
            }
        });
        
        wsService.on('message', (data) => {
            console.log('ðŸ“¥ WebSocket message received:', data);
            if (data.type === 'ai_response' && data.text) {
                addMessageToConversation(activeConversation, { 
                    text: data.text, 
                    isUser: false, 
                    timestamp: new Date() 
                });
            }
            if (data.type === 'audio_chunk' && data.audio) {
                // Play TTS audio response
                playBase64Audio(data.audio);
            }
        });
        
        wsService.on('error', (error) => {
            console.error('âŒ WebSocket error:', error);
            addMessageToConversation(activeConversation, { 
                text: "Connection error. Please try again. ðŸ”„", 
                isUser: false, 
                timestamp: new Date() 
            });
        });
        
        // Cleanup function
        return () => {
            // No explicit cleanup needed as services are singletons
        };

        // Handle AI responses
        service.on('ai_response', (data) => {
            console.log('ðŸ“¥ Received AI response:', data);
            console.log('ðŸ“¥ Response has text:', !!data.text);
            console.log('ðŸ“¥ Response has audio:', !!data.audio);
            console.log('ðŸ“¥ Response type:', data.type);
            console.log('ðŸ“¥ Full response keys:', Object.keys(data));
            
            if (data.text) {
                console.log('ðŸ“ Processing text response:', data.text);
                const aiMessage = { 
                    text: data.text, 
                    isUser: false, 
                    timestamp: new Date() 
                };
                addMessageToConversation(activeConversation, aiMessage);
                
                // Generate TTS if needed
                if (data.needsTTS) {
                    console.log('ðŸŽ¤ Generating TTS for response...');
                    generateAndPlayTTS(data.text, data.voiceId || '8LVfoRdkh4zgjr8v5ObE');
                }
            }
            
            // Handle different types of audio responses
            if (data.type === 'audio_chunk') {
                console.log(`ðŸ”Š Processing audio chunk ${data.chunkIndex + 1}/${data.totalChunks}`);
                // Store audio chunks for later assembly
                if (!(window as any).audioChunks) (window as any).audioChunks = [];
                (window as any).audioChunks.push(data.audio);
                
            } else if (data.type === 'audio_complete') {
                console.log('ðŸ”Š Audio complete, assembling chunks...');
                if ((window as any).audioChunks && (window as any).audioChunks.length > 0) {
                    const completeAudio = (window as any).audioChunks.join('');
                    (window as any).audioChunks = []; // Clear for next response
                    
                    try {
                        const audioBlob = new Blob(
                            [Uint8Array.from(atob(completeAudio), c => c.charCodeAt(0))], 
                            { type: 'audio/mpeg' }
                        );
                        console.log('ðŸ”Š Complete audio blob created, size:', audioBlob.size);
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        
                        audio.onloadstart = () => console.log('ðŸ”Š Audio loading started');
                        audio.oncanplay = () => console.log('ðŸ”Š Audio can play');
                        audio.onplay = () => console.log('ðŸ”Š Audio playback started');
                        audio.onended = () => {
                            console.log('ðŸ”Š Audio playback ended');
                            URL.revokeObjectURL(audioUrl);
                        };
                        audio.onerror = (e) => console.error('ðŸ”Š Audio error:', e);
                        
                        audio.play().then(() => {
                            console.log('ðŸ”Š Audio play() succeeded');
                        }).catch(error => {
                            console.error('âŒ Failed to play audio:', error);
                        });
                    } catch (error) {
                        console.error('âŒ Error processing complete audio:', error);
                    }
                }
                
            } else if (data.audio) {
                // Handle single audio response (non-streaming)
                console.log('ðŸ”Š Processing single audio response, length:', data.audio.length);
                try {
                    const audioBlob = new Blob(
                        [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))], 
                        { type: 'audio/mpeg' }
                    );
                    console.log('ðŸ”Š Audio blob created, size:', audioBlob.size);
                    const audioUrl = URL.createObjectURL(audioBlob);
                    const audio = new Audio(audioUrl);
                    
                    audio.onloadstart = () => console.log('ðŸ”Š Audio loading started');
                    audio.oncanplay = () => console.log('ðŸ”Š Audio can play');
                    audio.onplay = () => console.log('ðŸ”Š Audio playback started');
                    audio.onended = () => {
                        console.log('ðŸ”Š Audio playback ended');
                        URL.revokeObjectURL(audioUrl);
                    };
                    audio.onerror = (e) => console.error('ðŸ”Š Audio error:', e);
                    
                    audio.play().then(() => {
                        console.log('ðŸ”Š Audio play() succeeded');
                    }).catch(error => {
                        console.error('âŒ Failed to play audio:', error);
                    });
                } catch (error) {
                    console.error('âŒ Error processing audio response:', error);
                }
            } else {
                console.log('âš ï¸ No audio in response');
            }
        });

        // Handle status updates
        service.on('status', (data) => {
            console.log('ðŸ“Š Status update:', data);
            console.log('ðŸ“Š Current mode during status update:', mode);
            console.log('ðŸ“Š Active conversation ID:', activeConversation);
            console.log('ðŸ“Š Service has active conversation:', service.hasActiveConversation);
            
            if (data.type === 'connected') {
                console.log('âœ… Service connected - conversation ready');
                console.log('âœ… Mode should be conversing:', mode === 'conversing');
            } else if (data.type === 'disconnected') {
                console.log('âŒ Service disconnected');
            } else if (data.type === 'error') {
                console.log('âŒ Service error:', data);
            }
        });

        // Handle errors
        service.on('error', (data) => {
            console.error('Conversation error:', data);
            addMessageToConversation(activeConversation, { 
                text: "Connection error. Please try again.", 
                isUser: false, 
                timestamp: new Date() 
            });
        });

        // Cleanup on unmount
        return () => {
            if (service.hasActiveConversation) {
                service.endConversation();
            }
        };
    }, [activeConversation]);

    // Enhanced AI Agency Functions - COMPREHENSIVE AGENCY
    const analyzeCommand = (text: string): { type: 'navigation' | 'settings' | 'chat' | 'info' | 'agent' | 'game' | 'accessibility' | 'development' | 'content' | 'read', action?: any } => {
        const lowerText = text.toLowerCase();
        
        // === PROACTIVE ACCESSIBILITY SUPPORT ===
        // Color blindness detection and comprehensive assistance
        const colorBlindnessKeywords = ['color blind', 'colorblind', 'color blindness', 'can\'t see colors', 'trouble with colors', 'colors look the same', 'colors look similar', 'red green', 'blue yellow'];
        if (colorBlindnessKeywords.some(keyword => lowerText.includes(keyword))) {
            return { type: 'accessibility', action: { 
                type: 'color_blindness_support',
                step: 'initial_assessment',
                message: 'ðŸŽ¨ I can help you with color vision! Let me set up some tests to determine what type of color vision you have and customize NeuraPlay for you.',
                followUpActions: ['test_colors', 'determine_type', 'implement_settings']
            }};
        }
        
        // === DEVELOPMENT & LEARNING ASSISTANCE ===
        const developmentKeywords = ['help me learn', 'improve', 'get better at', 'develop', 'practice', 'struggle with', 'need help with', 'want to learn'];
        const skillKeywords = {
            'math': ['math', 'numbers', 'counting', 'addition', 'subtraction', 'arithmetic', 'calculation'],
            'reading': ['reading', 'letters', 'words', 'spelling', 'phonics', 'literacy'],
            'memory': ['memory', 'remember', 'forget', 'concentration', 'focus', 'attention'],
            'coordination': ['coordination', 'motor skills', 'balance', 'dexterity', 'hand-eye'],
            'problem_solving': ['problem solving', 'thinking', 'logic', 'puzzles', 'reasoning'],
            'inhibition': ['self control', 'focus', 'attention', 'impulse control', 'concentration'],
            'spatial': ['spatial', '3d', 'shapes', 'geometry', 'visual', 'patterns']
        };
        
        if (developmentKeywords.some(keyword => lowerText.includes(keyword))) {
            for (const [skill, keywords] of Object.entries(skillKeywords)) {
                if (keywords.some(keyword => lowerText.includes(keyword))) {
                    return { type: 'development', action: { 
                        skill, 
                        request: 'recommend_games',
                        message: `ðŸŽ® I have perfect games to help you with ${skill}! Let me show you our specialized activities.`
                    }};
                }
            }
            return { type: 'development', action: { 
                skill: 'general', 
                request: 'assess_needs',
                message: 'ðŸŒŸ I can help you improve! What specific skills would you like to work on?'
            }};
        }
        
        // === CONTENT CREATION AGENCY ===
        const creationKeywords = ['create', 'make', 'add', 'write', 'post', 'schedule', 'new'];
        if (creationKeywords.some(keyword => lowerText.includes(keyword))) {
            if (lowerText.includes('diary') || lowerText.includes('journal') || lowerText.includes('reflection')) {
                return { type: 'content', action: { 
                    type: 'diary', 
                    request: 'create_prompt',
                    message: 'ðŸ“ I\'ll create a personalized diary prompt for you! Let me set that up.'
                }};
            } else if (lowerText.includes('calendar') || lowerText.includes('event') || lowerText.includes('reminder') || lowerText.includes('schedule')) {
                return { type: 'content', action: { 
                    type: 'calendar', 
                    request: 'create_entry',
                    message: 'ðŸ“… I\'ll help you create a calendar entry! What would you like to schedule?'
                }};
            } else if (lowerText.includes('forum') || lowerText.includes('discussion') || lowerText.includes('community') || lowerText.includes('post')) {
                return { type: 'content', action: { 
                    type: 'forum', 
                    request: 'create_post',
                    message: 'ðŸ’¬ I\'ll help you create a forum post! What would you like to discuss?'
                }};
            }
        }
        
        // === INFORMATION READING AGENCY ===
        const readingKeywords = ['read', 'show', 'what\'s in', 'check', 'look at', 'see', 'view', 'display'];
        if (readingKeywords.some(keyword => lowerText.includes(keyword))) {
            if (lowerText.includes('notification') || lowerText.includes('alert') || lowerText.includes('message')) {
                return { type: 'read', action: { 
                    type: 'notifications',
                    message: 'ðŸ”” Let me check your notifications for you!'
                }};
            } else if (lowerText.includes('forum') || lowerText.includes('discussion') || lowerText.includes('community')) {
                return { type: 'read', action: { 
                    type: 'forum',
                    message: 'ðŸ’¬ I\'ll show you the latest forum discussions!'
                }};
            } else if (lowerText.includes('diary') || lowerText.includes('journal') || lowerText.includes('reflection')) {
                return { type: 'read', action: { 
                    type: 'diary',
                    message: 'ðŸ“– Let me show you your diary entries!'
                }};
            } else if (lowerText.includes('playground') || lowerText.includes('games') || lowerText.includes('activities')) {
                return { type: 'read', action: { 
                    type: 'playground',
                    message: 'ðŸŽ® I\'ll show you all available games and activities!'
                }};
            }
        }
        
        // ENHANCED AI Agent commands with better natural language processing
        const agentKeywords = [
            'ai agent', 'agent', 'ai assistant', 'assistant', 'brain', 'synapse', 
            'ai teacher', 'teacher', 'ai', 'bot', 'helper', 'guide', 'tutor'
        ];
        
        const agentActionKeywords = [
            'show', 'open', 'display', 'bring', 'activate', 'hide', 'close', 'remove', 
            'dismiss', 'stop', 'trigger', 'start', 'begin', 'launch', 'wake', 'call',
            'summon', 'appear', 'disappear', 'go away', 'leave', 'come', 'help'
        ];
        
        const agentIntentKeywords = [
            'want', 'need', 'like', 'would like', 'can you', 'please', 'help me',
            'i want', 'i need', 'i would like', 'make it', 'put it', 'set it'
        ];
        
        // Enhanced agent detection - MUCH MORE SPECIFIC
        const hasAgentIntent = (
            // Must have explicit agent keywords AND action words
            (agentKeywords.some(keyword => lowerText.includes(keyword)) && 
             agentActionKeywords.some(action => lowerText.includes(action))) ||
            // OR very specific command patterns
            lowerText.includes('ai agent') || 
            lowerText.includes('ai assistant') ||
            lowerText.includes('show agent') ||
            lowerText.includes('hide agent') ||
            lowerText.includes('trigger agent')
        );
        
        if (hasAgentIntent) {
            // Show/hide agent commands with enhanced variations
            if (lowerText.includes('show') || lowerText.includes('open') || lowerText.includes('display') || 
                lowerText.includes('bring') || lowerText.includes('activate') || lowerText.includes('wake') ||
                lowerText.includes('summon') || lowerText.includes('appear') || lowerText.includes('come') ||
                lowerText.includes('help') || lowerText.includes('start')) {
                return { type: 'agent', action: { command: 'show' } };
            }
            
            if (lowerText.includes('hide') || lowerText.includes('close') || lowerText.includes('remove') || 
                lowerText.includes('dismiss') || lowerText.includes('stop') || lowerText.includes('go away') ||
                lowerText.includes('leave') || lowerText.includes('disappear')) {
                return { type: 'agent', action: { command: 'hide' } };
            }
            
            // Enhanced trigger agent commands with more variations
            if (lowerText.includes('trigger') || lowerText.includes('start') || lowerText.includes('begin') || 
                lowerText.includes('launch') || lowerText.includes('activate')) {
                let triggerType = 'manual';
                if (lowerText.includes('achievement') || lowerText.includes('success') || lowerText.includes('win')) {
                    triggerType = 'achievement';
                } else if (lowerText.includes('struggle') || lowerText.includes('difficulty') || lowerText.includes('problem') || lowerText.includes('stuck')) {
                    triggerType = 'struggle';
                } else if (lowerText.includes('milestone') || lowerText.includes('progress') || lowerText.includes('step')) {
                    triggerType = 'milestone';
                } else if (lowerText.includes('auto') || lowerText.includes('automatic')) {
                    triggerType = 'auto';
                } else if (lowerText.includes('stuck') || lowerText.includes('repeating')) {
                    triggerType = 'stuck';
                } else if (lowerText.includes('rapid') || lowerText.includes('fast') || lowerText.includes('quick')) {
                    triggerType = 'rapid';
                }
                
                return { type: 'agent', action: { command: 'trigger', triggerType } };
            }
            
            // Enhanced personality commands with more natural language
            const personalityKeywords = [
                'coach', 'mentor', 'friend', 'analyst', 'teacher', 'guide', 'buddy', 'pal',
                'instructor', 'advisor', 'companion', 'helper', 'tutor', 'trainer'
            ];
            
            const personality = personalityKeywords.find(keyword => lowerText.includes(keyword));
            if (personality) {
                let personalityType = personality;
                if (personality === 'teacher' || personality === 'instructor' || personality === 'trainer') {
                    personalityType = 'coach';
                } else if (personality === 'guide' || personality === 'advisor') {
                    personalityType = 'mentor';
                } else if (personality === 'buddy' || personality === 'pal' || personality === 'companion') {
                    personalityType = 'friend';
                }
                
                return { type: 'agent', action: { command: 'personality', personality: personalityType } };
            }
            
            // Default agent command - show agent if no specific action detected
            if (agentKeywords.some(keyword => lowerText.includes(keyword))) {
                return { type: 'agent', action: { command: 'show' } };
            }
        }
        
        // Enhanced Navigation commands - MUCH MORE SPECIFIC
        // ONLY block pure greetings that have no other intent
        const pureGreetings = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening', 'yo', 'greetings', 'goodbye', 'bye'];
        
        // *** PRIORITY 1: NAVIGATION COMMANDS ***
        const navigationKeywords = ['go to', 'take me to', 'navigate to', 'open', 'show me', 'visit'];
        const hasNavigationKeyword = navigationKeywords.some(keyword => lowerText.includes(keyword));
        
        // Direct navigation phrases that should ALWAYS work
        const directNavigation = 
            lowerText.includes('take me to dashboard') ||
            lowerText.includes('take me to forum') ||
            lowerText.includes('take me to playground') ||
            lowerText.includes('go to dashboard') ||
            lowerText.includes('go to forum') ||
            lowerText.includes('go to playground') ||
            (hasNavigationKeyword && (
                lowerText.includes('dashboard') ||
                lowerText.includes('forum') ||
                lowerText.includes('playground') ||
                lowerText.includes('profile') ||
                lowerText.includes('settings') ||
                lowerText.includes('about')
            ));
        
        // *** EXECUTE NAVIGATION COMMANDS IMMEDIATELY ***
        if (directNavigation) {
            console.log('ðŸŽ¯ DIRECT NAVIGATION DETECTED:', lowerText);
            
            // Match specific pages with priority order
            if (lowerText.includes('dashboard')) {
                console.log('âœ… Navigation to DASHBOARD');
                return { type: 'navigation', action: { path: '/dashboard', page: availablePages['/dashboard'] } };
            }
            if (lowerText.includes('forum')) {
                console.log('âœ… Navigation to FORUM');  
                return { type: 'navigation', action: { path: '/forum', page: availablePages['/forum'] } };
            }
            if (lowerText.includes('playground')) {
                console.log('âœ… Navigation to PLAYGROUND');
                return { type: 'navigation', action: { path: '/playground', page: availablePages['/playground'] } };
            }
            if (lowerText.includes('profile')) {
                return { type: 'navigation', action: { path: '/profile', page: availablePages['/profile'] } };
            }
            if (lowerText.includes('about')) {
                return { type: 'navigation', action: { path: '/about', page: availablePages['/about'] } };
            }
            
            // Enhanced special navigation cases - ALL PAGES
            if (lowerText.includes('home') || lowerText.includes('main') || lowerText.includes('landing')) {
                return { type: 'navigation', action: { path: '/', page: availablePages['/'] } };
            }
            
            if (lowerText.includes('playground') || lowerText.includes('games') || lowerText.includes('activities')) {
                return { type: 'navigation', action: { path: '/playground', page: availablePages['/playground'] } };
            }
            
            if (lowerText.includes('learning central') || lowerText.includes('progress') || lowerText.includes('dashboard') || lowerText.includes('stats') || lowerText.includes('statistics')) {
                return { type: 'navigation', action: { path: '/dashboard', page: availablePages['/dashboard'] } };
            }
            
            if (lowerText.includes('forum') || lowerText.includes('community') || lowerText.includes('discussions')) {
                return { type: 'navigation', action: { path: '/forum', page: availablePages['/forum'] } };
            }
            
            if (lowerText.includes('register') || lowerText.includes('sign up') || lowerText.includes('create account')) {
                return { type: 'navigation', action: { path: '/registration', page: availablePages['/registration'] } };
            }
            
            if (lowerText.includes('sign in') || lowerText.includes('login') || lowerText.includes('log in')) {
                return { type: 'navigation', action: { path: '/signin', page: availablePages['/signin'] } };
            }
            
            if (lowerText.includes('ai report') || lowerText.includes('analytics') || lowerText.includes('ai analytics')) {
                return { type: 'navigation', action: { path: '/ai-report', page: availablePages['/ai-report'] } };
            }
            
            if (lowerText.includes('about') || lowerText.includes('about us')) {
                return { type: 'navigation', action: { path: '/about', page: availablePages['/about'] } };
            }
            
            if (lowerText.includes('counting') || lowerText.includes('math') || lowerText.includes('numbers')) {
                return { type: 'navigation', action: { path: '/counting-test', page: availablePages['/counting-test'] } };
            }
            
            if (lowerText.includes('test') || lowerText.includes('testing')) {
                return { type: 'navigation', action: { path: '/test', page: availablePages['/test'] } };
            }
            
            if (lowerText.includes('text reveal') || lowerText.includes('animations') || lowerText.includes('text animations')) {
                return { type: 'navigation', action: { path: '/text-reveal', page: availablePages['/text-reveal'] } };
            }
            
            if (lowerText.includes('profile') || lowerText.includes('user profile')) {
                return { type: 'navigation', action: { path: '/profile', page: availablePages['/profile'] } };
            }
            
            if (lowerText.includes('old home') || lowerText.includes('previous home')) {
                return { type: 'navigation', action: { path: '/old-home', page: availablePages['/old-home'] } };
            }
        }
        
        // Settings commands with expanded variations
        const settingsKeywords = ['settings', 'preferences', 'options', 'config', 'setup'];
        if (settingsKeywords.some(keyword => lowerText.includes(keyword))) {
            return { type: 'settings', action: 'open' };
        }
        
        // *** PRIORITY 2: THEME COMMANDS ***
        // Detect theme changes with natural language
        if ((lowerText.includes('turn on') || lowerText.includes('switch to') || lowerText.includes('enable')) && lowerText.includes('dark')) {
            return { type: 'settings', action: { setting: 'theme', value: 'dark' } };
        }
        if ((lowerText.includes('turn on') || lowerText.includes('switch to') || lowerText.includes('enable')) && lowerText.includes('light')) {
            return { type: 'settings', action: { setting: 'theme', value: 'light' } };
        }
        
        // General theme keywords
        const themeKeywords = ['theme', 'dark', 'light', 'mode', 'appearance'];
        const themeActionKeywords = ['make', 'put', 'bring', 'set', 'change', 'switch', 'turn', 'enable', 'use'];
        
        if (themeKeywords.some(keyword => lowerText.includes(keyword)) || 
            (themeActionKeywords.some(action => lowerText.includes(action)) && (lowerText.includes('dark') || lowerText.includes('light')))) {
            let themeValue = 'auto';
            if (lowerText.includes('dark')) themeValue = 'dark';
            else if (lowerText.includes('light')) themeValue = 'light';
            return { type: 'settings', action: { setting: 'theme', value: themeValue } };
        }
        
        // Font size settings with expanded variations
        const fontKeywords = ['font', 'text size', 'text', 'size', 'bigger', 'smaller', 'spacing'];
        const fontActionKeywords = ['make', 'put', 'bring', 'set', 'change', 'increase', 'decrease'];
        
        if (fontKeywords.some(keyword => lowerText.includes(keyword)) || 
            (fontActionKeywords.some(action => lowerText.includes(action)) && (lowerText.includes('large') || lowerText.includes('small') || lowerText.includes('big') || lowerText.includes('tiny') || lowerText.includes('wider') || lowerText.includes('spacing')))) {
            let size = 'medium';
            if (lowerText.includes('large') || lowerText.includes('big') || lowerText.includes('bigger') || lowerText.includes('wider')) {
                size = 'large';
            } else if (lowerText.includes('small') || lowerText.includes('tiny') || lowerText.includes('smaller')) {
                size = 'small';
            }
            return { type: 'settings', action: { setting: 'fontSize', value: size } };
        }
        
        // Animation settings with expanded variations
        const animationKeywords = ['animation', 'motion', 'effects', 'transitions'];
        const animationActionKeywords = ['make', 'put', 'bring', 'set', 'change', 'enable', 'disable', 'turn'];
        
        if (animationKeywords.some(keyword => lowerText.includes(keyword)) || 
            (animationActionKeywords.some(action => lowerText.includes(action)) && (lowerText.includes('animation') || lowerText.includes('motion')))) {
            const enabled = !lowerText.includes('disable') && !lowerText.includes('off') && !lowerText.includes('stop');
            return { type: 'settings', action: { setting: 'animations', value: enabled ? 'enabled' : 'disabled' } };
        }
        
        // Sound settings with expanded variations
        const soundKeywords = ['sound', 'audio', 'volume', 'noise', 'voice'];
        const soundActionKeywords = ['make', 'put', 'bring', 'set', 'change', 'enable', 'disable', 'turn', 'mute'];
        
        if (soundKeywords.some(keyword => lowerText.includes(keyword)) || 
            (soundActionKeywords.some(action => lowerText.includes(action)) && (lowerText.includes('sound') || lowerText.includes('audio') || lowerText.includes('voice')))) {
            const enabled = !lowerText.includes('mute') && !lowerText.includes('off') && !lowerText.includes('disable');
            return { type: 'settings', action: { setting: 'sound', value: enabled ? 'enabled' : 'disabled' } };
        }

        // AI Agent settings
        const aiAgentSettingKeywords = ['ai agent', 'agent', 'ai assistant', 'assistant'];
        if (aiAgentSettingKeywords.some(keyword => lowerText.includes(keyword))) {
            const enabled = !lowerText.includes('disable') && !lowerText.includes('off') && !lowerText.includes('stop');
            return { type: 'settings', action: { setting: 'aiAgent', value: enabled ? 'enabled' : 'disabled' } };
        }

        // Notifications settings
        const notificationKeywords = ['notification', 'notify', 'alert', 'reminder'];
        if (notificationKeywords.some(keyword => lowerText.includes(keyword))) {
            const enabled = !lowerText.includes('disable') && !lowerText.includes('off') && !lowerText.includes('stop');
            return { type: 'settings', action: { setting: 'notifications', value: enabled ? 'enabled' : 'disabled' } };
        }

        // Auto save settings
        const autoSaveKeywords = ['auto save', 'autosave', 'save'];
        if (autoSaveKeywords.some(keyword => lowerText.includes(keyword))) {
            const enabled = !lowerText.includes('disable') && !lowerText.includes('off') && !lowerText.includes('stop');
            return { type: 'settings', action: { setting: 'autoSave', value: enabled ? 'enabled' : 'disabled' } };
        }

        // Enhanced Accessibility settings - ALL ACCESSIBILITY FEATURES
        const accessibilityKeywords = [
            'colorblind', 'color blind', 'protanopia', 'deuteranopia', 'tritanopia', 
            'contrast', 'spacing', 'bold', 'text spacing', 'high contrast', 'accessibility',
            'screen reader', 'keyboard navigation', 'focus indicators', 'reduced motion'
        ];
        const accessibilityActionKeywords = ['make', 'put', 'bring', 'set', 'change', 'enable', 'disable', 'turn', 'use', 'need', 'want', 'have'];
        
        if (accessibilityKeywords.some(keyword => lowerText.includes(keyword)) || 
            (accessibilityActionKeywords.some(action => lowerText.includes(action)) && 
             (lowerText.includes('colorblind') || lowerText.includes('contrast') || lowerText.includes('spacing') || 
              lowerText.includes('bold') || lowerText.includes('accessibility') || lowerText.includes('screen reader') || 
              lowerText.includes('keyboard') || lowerText.includes('focus') || lowerText.includes('motion')))) {
            
            // Handle ALL accessibility requests
            if (lowerText.includes('protanopia') || lowerText.includes('deuteranopia') || lowerText.includes('tritanopia')) {
                return { type: 'settings', action: { setting: 'colorBlindMode', value: lowerText.includes('protanopia') ? 'protanopia' : lowerText.includes('deuteranopia') ? 'deuteranopia' : 'tritanopia' } };
            }
            
            if (lowerText.includes('high contrast') || lowerText.includes('contrast')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'highContrast', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            if (lowerText.includes('screen reader')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'screenReader', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            if (lowerText.includes('keyboard navigation') || lowerText.includes('keyboard')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'keyboardNavigation', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            if (lowerText.includes('focus indicators') || lowerText.includes('focus')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'focusIndicators', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            if (lowerText.includes('reduced motion') || lowerText.includes('motion')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'reducedMotion', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            if (lowerText.includes('spacing') || lowerText.includes('space') || lowerText.includes('bold')) {
                if (lowerText.includes('extra') || lowerText.includes('more')) {
                    return { type: 'settings', action: { setting: 'textSpacing', value: 'extra' } };
                } else if (lowerText.includes('increased') || lowerText.includes('more')) {
                    return { type: 'settings', action: { setting: 'textSpacing', value: 'increased' } };
                } else {
                    return { type: 'settings', action: { setting: 'textSpacing', value: 'normal' } };
                }
            }
            
            if (lowerText.includes('accessibility')) {
                const enabled = !lowerText.includes('disable') && !lowerText.includes('off');
                return { type: 'settings', action: { setting: 'accessibility', value: enabled ? 'enabled' : 'disabled' } };
            }
            
            // Default to chat for complex accessibility requests
            return { type: 'chat' };
        }
        
        // Info commands
        if (lowerText.includes('what can you do') || lowerText.includes('help') || lowerText.includes('capabilities')) {
            return { type: 'info', action: 'capabilities' };
        }
        
        if (lowerText.includes('where am i') || lowerText.includes('current page')) {
            return { type: 'info', action: 'location' };
        }
        
        // === ENHANCED GAME RECOMMENDATIONS ===
        const gameKeywords = ['game', 'play', 'activity', 'exercise', 'fun'];
        if (gameKeywords.some(keyword => lowerText.includes(keyword))) {
            return { type: 'game', action: { 
                request: 'recommend',
                message: 'ðŸŽ® Let me recommend perfect games for you based on your interests!'
            }};
        }
        
        // Default to chat with proactive analysis
        return { type: 'chat' };
    };

    const executeCommand = async (command: { type: string, action?: any }): Promise<string> => {
        switch (command.type) {
            case 'accessibility':
                if (command.action?.type === 'color_blindness_support') {
                    // Navigate to a test page or create color tests
                    navigate('/playground');
                    setTimeout(() => {
                        addMessageToConversation(activeConversation, {
                            text: `ðŸŽ¨ I've taken you to the playground where we can test your color vision! I'm going to create some color tests for you. 

Let's start: Can you tell me if these colors look the same or different?
ðŸ”´ Red vs ðŸŸ¢ Green
ðŸ”µ Blue vs ðŸŸ¡ Yellow

Based on your answers, I'll customize NeuraPlay's colors to work perfectly for you!`,
                            isUser: false,
                            timestamp: new Date()
                        });
                    }, 1000);
                    return command.action.message;
                }
                
                if (command.action?.type === 'color_blindness_analysis') {
                    // Apply colorblind settings based on analysis
                    if (command.action.result === 'red_green_deficiency') {
                        // Enable colorblind mode
                        console.log('ðŸŽ¨ Enabling colorblind-friendly mode');
                        // Add actual colorblind mode implementation here
                    }
                    return command.action.message;
                }
                break;
                
            case 'development':
                if (command.action?.request === 'recommend_games') {
                    const skill = command.action.skill;
                    const recommendedGames = Object.entries(availableGames).filter(([key, game]) => {
                        const benefits = game.benefits.toLowerCase();
                        switch(skill) {
                            case 'math': return benefits.includes('math') || benefits.includes('number') || benefits.includes('count');
                            case 'reading': return benefits.includes('letter') || benefits.includes('reading') || benefits.includes('literacy');
                            case 'memory': return benefits.includes('memory') || benefits.includes('attention');
                            case 'coordination': return benefits.includes('coordination') || benefits.includes('motor');
                            case 'problem_solving': return benefits.includes('problem') || benefits.includes('reasoning');
                            case 'inhibition': return benefits.includes('control') || benefits.includes('attention');
                            case 'spatial': return benefits.includes('spatial') || benefits.includes('visual');
                            default: return true;
                        }
                    });
                    
                    navigate('/playground');
                    let gameList = recommendedGames.map(([key, game]) => 
                        `â€¢ **${game.name}**: ${game.description}\n  *Benefits: ${game.benefits}*`
                    ).join('\n\n');
                    
                    return `${command.action.message}\n\nðŸŽ¯ **Perfect games for ${skill} development:**\n\n${gameList}\n\nI've taken you to the playground - try these games!`;
                }
                break;
                
            case 'content':
                if (command.action?.type === 'diary') {
                    const prompts = [
                        "What was the most interesting thing you learned today?",
                        "Describe a challenge you overcame recently and how you did it.",
                        "What are three things you're grateful for right now?",
                        "If you could teach someone else something you learned, what would it be?",
                        "What goal do you want to work on tomorrow?"
                    ];
                    const randomPrompt = prompts[Math.floor(Math.random() * prompts.length)];
                    
                    return `ðŸ“ Here's your personalized diary prompt:\n\n"${randomPrompt}"\n\nTake your time to reflect and write your thoughts! ðŸŒŸ`;
                } else if (command.action?.type === 'calendar') {
                    return `ðŸ“… I'm ready to help you schedule something! What would you like to add to your calendar? For example:
â€¢ Study session
â€¢ Game time  
â€¢ Practice activity
â€¢ Learning goal
â€¢ Fun activity

Just tell me what and when! ðŸ—“ï¸`;
                } else if (command.action?.type === 'forum') {
                    navigate('/forum');
                    return `ðŸ’¬ I've taken you to the forum! You can create a post about:
â€¢ Ask questions about games
â€¢ Share your progress
â€¢ Help other learners
â€¢ Discuss learning strategies
â€¢ Connect with the community

What would you like to post about? ðŸŒŸ`;
                }
                break;
                
            case 'read':
                if (command.action?.type === 'notifications') {
                    return `ðŸ”” Here are your recent notifications:\n\nâ€¢ Welcome to NeuraPlay! ðŸŒŸ\nâ€¢ New games available in playground\nâ€¢ Daily learning streak: Keep it up!\nâ€¢ Community forum has new discussions\n\nI can also help you customize notification settings if needed! ðŸ“±`;
                } else if (command.action?.type === 'forum') {
                    navigate('/forum');
                    return `ðŸ’¬ I've taken you to the forum! Here's what's happening:\n\nâ€¢ Active discussions about learning strategies\nâ€¢ Users sharing their progress\nâ€¢ Questions and answers about games\nâ€¢ Community challenges and events\n\nYou can join any discussion or start your own! ðŸŒŸ`;
                } else if (command.action?.type === 'playground') {
                    navigate('/playground');
                    const gamesList = Object.entries(availableGames).map(([key, game]) => 
                        `â€¢ **${game.name}**: ${game.description}`
                    ).join('\n');
                    return `ðŸŽ® Here are all available games and activities:\n\n${gamesList}\n\nI've taken you to the playground - have fun learning! ðŸŒŸ`;
                } else if (command.action?.type === 'diary') {
                    return `ðŸ“– Your diary entries:\n\nâ€¢ Reflections on learning progress\nâ€¢ Daily thoughts and insights\nâ€¢ Goals and achievements\nâ€¢ Challenges and solutions\n\nWould you like me to create a new diary prompt for today? ðŸ“`;
                }
                break;
            case 'agent':
                if (command.action?.command === 'show') {
                    try {
                        // Create a context for the agent with current game state
                        const agentContext = {
                            gameId: currentContext?.gameId || 'general',
                            currentProgress: currentContext?.currentProgress || 0,
                            moveHistory: currentContext?.moveHistory || [],
                            timeSpent: currentContext?.timeSpent || 0,
                            agentPersonality: currentContext?.agentPersonality || 'synapse-normal'
                        };
                        showAgent(agentContext);
                        return `ðŸ¤– AI Agent activated! I've opened the AI assistant for you! ðŸ§ âœ¨`;
                    } catch (error) {
                        console.error('Agent show error:', error);
                        return `âŒ Sorry, I couldn't activate the AI agent right now. Please try again! ðŸ¤–`;
                    }
                }
                
                if (command.action?.command === 'hide') {
                    try {
                        hideAgent();
                        return `ðŸ¤– AI Agent hidden! The AI assistant is now closed! ðŸ‘‹`;
                    } catch (error) {
                        console.error('Agent hide error:', error);
                        return `âŒ Sorry, I couldn't hide the AI agent right now. Please try again! ðŸ¤–`;
                    }
                }
                
                if (command.action?.command === 'trigger') {
                    try {
                        const triggerType = command.action.triggerType || 'manual';
                        // Create a context for the agent with current game state
                        const agentContext = {
                            gameId: currentContext?.gameId || 'general',
                            currentProgress: currentContext?.currentProgress || 0,
                            moveHistory: currentContext?.moveHistory || [],
                            timeSpent: currentContext?.timeSpent || 0,
                            agentPersonality: currentContext?.agentPersonality || 'synapse-normal'
                        };
                        triggerAgent(triggerType as any, agentContext);
                        return `ðŸš€ AI Agent triggered with ${triggerType} mode! The AI assistant is now active! âš¡`;
                    } catch (error) {
                        console.error('Agent trigger error:', error);
                        return `âŒ Sorry, I couldn't trigger the AI agent right now. Please try again! ðŸ¤–`;
                    }
                }
                
                if (command.action?.command === 'personality') {
                    try {
                        const personality = command.action.personality;
                        updateContext({ agentPersonality: personality });
                        
                        // Apply personality to current conversation
                        const personalityMessages = {
                            coach: "ðŸŽ¯ I'm now in Coach mode! I'll be direct, encouraging, and help you achieve your goals! Let's get you to the next level! ðŸ’ª",
                            mentor: "ðŸ§  I'm now in Mentor mode! I'll guide you with wisdom and help you understand concepts deeply! Let's explore together! ðŸŒŸ",
                            friend: "ðŸ˜Š I'm now in Friend mode! I'll be supportive, fun, and here to chat with you! What's on your mind? ðŸ’¬",
                            analyst: "ðŸ” I'm now in Analyst mode! I'll help you understand data, patterns, and make informed decisions! Let's analyze! ðŸ”"
                        };
                        
                        return `ðŸŽ­ AI Agent personality changed to ${personality}! ${personalityMessages[personality as keyof typeof personalityMessages] || 'The AI assistant will now behave differently!'} ðŸŽ¨`;
                    } catch (error) {
                        console.error('Agent personality error:', error);
                        return `âŒ Sorry, I couldn't change the AI personality right now. Please try again! ðŸ¤–`;
                    }
                }
                break;
                
            case 'navigation':
                if (command.action?.path) {
                    console.log('Navigating to:', command.action.path, command.action.page);
                    navigate(command.action.path);
                    return `ðŸš€ Taking you to ${command.action.page.name}! ${command.action.page.description} âœ¨`;
                }
                break;
                
            case 'settings':
                if (command.action === 'open') {
                    return `âš™ï¸ Settings panel is now available! You can customize your experience through the settings dropdown in the header! ðŸŽ›ï¸`;
                }
                
                if (command.action?.setting) {
                    const { setting, value } = command.action;
                    try {
                        switch (setting) {
                            case 'theme':
                                theme.setTheme(value);
                                return `ðŸŽ¨ Theme changed to ${value}! The page should update automatically! âœ¨`;
                            case 'fontSize':
                                theme.setFontSize(value);
                                return `ðŸ“ Font size changed to ${value}! Text should be easier to read now! ðŸ“–`;
                            case 'animations':
                                theme.setAnimationsEnabled(value === 'enabled');
                                return `ðŸŽ¬ Animations ${value === 'enabled' ? 'enabled' : 'disabled'}! ${value === 'enabled' ? 'Things will be more lively!' : 'Things will be calmer!'} ðŸŽ­`;
                            case 'sound':
                                // This would need to be implemented in the theme context
                                return `ðŸ”Š Sound settings would be updated here! ðŸ”Š`;
                            case 'colorBlindMode':
                                theme.setColorBlindMode(value);
                                return `ðŸŒˆ Color blind mode set to ${value}! I've adjusted the colors to make them easier to see! ðŸŽ¨`;
                            case 'highContrast':
                                theme.setHighContrast(value === 'enabled');
                                return `ðŸ“– High contrast ${value === 'enabled' ? 'enabled' : 'disabled'}! Text should be much easier to read now! ðŸ‘ï¸`;
                            case 'textSpacing':
                                theme.setTextSpacing(value);
                                return `ðŸ“ Text spacing changed to ${value}! Letters should be easier to read now! ðŸ”¤`;
                            case 'aiAgent':
                                // This would control AI agent visibility/behavior
                                return `ðŸ¤– AI Agent ${value === 'enabled' ? 'enabled' : 'disabled'}! The AI assistant is now ${value === 'enabled' ? 'active' : 'inactive'}! ðŸ§ `;
                            case 'notifications':
                                // This would control notification settings
                                return `ðŸ”” Notifications ${value === 'enabled' ? 'enabled' : 'disabled'}! You'll ${value === 'enabled' ? 'receive' : 'not receive'} alerts! ðŸ“¢`;
                            case 'autoSave':
                                // This would control auto-save settings
                                return `ðŸ’¾ Auto save ${value === 'enabled' ? 'enabled' : 'disabled'}! Your progress will ${value === 'enabled' ? 'be saved automatically' : 'need manual saving'}! ðŸ’¿`;
                            case 'accessibility':
                                // This would control general accessibility settings
                                return `â™¿ Accessibility features ${value === 'enabled' ? 'enabled' : 'disabled'}! The platform is now ${value === 'enabled' ? 'more accessible' : 'less accessible'}! ðŸŽ¯`;
                            case 'reducedMotion':
                                theme.setReducedMotion(value === 'enabled');
                                return `ðŸŽ¬ Reduced motion ${value === 'enabled' ? 'enabled' : 'disabled'}! Animations will be ${value === 'enabled' ? 'minimized' : 'full'}! ðŸŽ­`;
                            case 'screenReader':
                                theme.setScreenReader(value === 'enabled');
                                return `ðŸ“– Screen reader ${value === 'enabled' ? 'enabled' : 'disabled'}! ${value === 'enabled' ? 'Screen readers will work better' : 'Screen reader support disabled'}! ðŸ”Š`;
                            case 'keyboardNavigation':
                                theme.setKeyboardNavigation(value === 'enabled');
                                return `âŒ¨ï¸ Keyboard navigation ${value === 'enabled' ? 'enabled' : 'disabled'}! You can ${value === 'enabled' ? 'navigate with keyboard' : 'use mouse/touch'}! ðŸŽ¯`;
                            case 'focusIndicators':
                                theme.setFocusIndicators(value === 'enabled');
                                return `ðŸŽ¯ Focus indicators ${value === 'enabled' ? 'enabled' : 'disabled'}! ${value === 'enabled' ? 'Focus will be clearly visible' : 'Focus indicators hidden'}! ðŸ‘ï¸`;
                        }
                    } catch (error) {
                        return `Oops! I couldn't change that setting right now. Try opening settings manually! âš™ï¸`;
                    }
                }
                break;
                
            case 'info':
                if (command.action === 'capabilities') {
                    const allPages = Object.entries(availablePages).map(([path, page]) => 
                        `â€¢ "${page.name}" or "Go to ${page.name}" or "Show ${page.name}"`
                    ).join('\n');
                    
                    const allSettings = Object.entries(availableSettings).map(([key, setting]) => {
                        return `â€¢ "${setting.name}" - ${setting.description}`;
                    }).join('\n');
                    
                    return `ðŸŒŸ Here's what I can do for you! ðŸš€

ðŸŽ® **Navigation**: I can take you to ANY page! Try:
${allPages}

âš™ï¸ **Settings**: I can change ALL your preferences! Try:
${allSettings}

ðŸ¤– **AI Agent Control**: I can control the AI assistant! Try:
â€¢ "Show AI agent" or "Open AI assistant" or "Activate AI" or "Wake up AI"
â€¢ "Hide AI agent" or "Close AI assistant" or "Stop AI" or "Go away AI"
â€¢ "Trigger AI agent" or "Start AI assistant" or "Launch AI" or "Call AI"
â€¢ "Change AI to coach" or "Set AI as mentor" or "Make AI friend" or "AI be analyst"
â€¢ "I want the AI agent" or "I need the assistant" or "Can you show the AI"
â€¢ "Bring the AI" or "Summon the agent" or "Wake the assistant"

ðŸŽ­ **AI Personalities**:
â€¢ "Be my coach" or "Act as a mentor" or "Be my friend" or "Analyze like an analyst"
â€¢ "Change to coach mode" or "Switch to mentor" or "Make it friendly" or "Be analytical"
â€¢ "I want a coach" or "I need a mentor" or "Be my buddy" or "Help me analyze"

ðŸš€ **AI Triggers**:
â€¢ "Trigger achievement mode" or "Activate on success" or "Trigger on win"
â€¢ "Trigger struggle mode" or "Activate on difficulty" or "Help when stuck"
â€¢ "Trigger milestone mode" or "Activate on progress" or "Celebrate steps"
â€¢ "Trigger auto mode" or "Activate automatically" or "Smart triggers"
â€¢ "Trigger stuck mode" or "Activate on repetition" or "Help with patterns"
â€¢ "Trigger rapid mode" or "Activate on fast moves" or "Help with speed"

ðŸŒˆ **Accessibility**: I can help with special needs! Try:
â€¢ "I am colorblind" or "I have protanopia"
â€¢ "Make high contrast" or "Put bold text"
â€¢ "Space letters out" or "Make text bigger"
â€¢ "I need help seeing" or "Make it easier to read"
â€¢ "Enable accessibility" or "Disable accessibility"
â€¢ "Enable screen reader" or "Disable screen reader"
â€¢ "Enable keyboard navigation" or "Disable keyboard navigation"
â€¢ "Enable focus indicators" or "Disable focus indicators"
â€¢ "Enable reduced motion" or "Disable reduced motion"

ðŸ’¬ **Chat**: I can answer questions, draw pictures, and help with learning!

ðŸ“ **Current Location**: You're currently on ${availablePages[location.pathname as keyof typeof availablePages]?.name || 'an unknown page'}!

Need help with anything specific? Just ask! ðŸŒŸ`;
                }
                
                if (command.action === 'location') {
                    const currentPage = availablePages[location.pathname as keyof typeof availablePages];
                    return `ðŸ“ You're currently on the ${currentPage?.name || 'unknown'} page! ${currentPage?.description || ''} ðŸŽ¯`;
                }
                break;
        }
        
        return `âŒ I didn't understand that command. Try asking for help!`;
    };

    // NEW: Unified message handling function
    const handleSendMessage = async (text: string) => {
        if (!text.trim() || isLoading) return;
        const currentMode = modeRef.current; // Use ref for more accurate mode
        console.log(`Sending message in mode: ${currentMode}`);

        // Add user message to UI
        const userMessage: Message = { text, isUser: true, timestamp: new Date() };
        addMessageToConversation(activeConversation, userMessage);
        setInputMessage('');
        setIsLoading(true);

        // Apply prompt count restrictions for non-demo users
        if (!isDemoUser) {
            setPromptCount(count => count + 1);
        }

        try {
            // In full conversation mode, always treat input as a chat message
            if (currentMode === 'conversing') {
                await handleConversationMode(text);
                // Auto-play the response
                setTimeout(async () => {
                    const messages = conversations[activeConversation]?.messages || [];
                    const userMessageTime = userMessage.timestamp;
                    const recentAIMessages = messages
                        .filter(msg => !msg.isUser && msg.timestamp > userMessageTime)
                        .sort((a, b) => b.timestamp.getTime() - a.timestamp.getTime());
                    
                    if (recentAIMessages.length > 0) {
                        const latestAIMessage = recentAIMessages[0];
                        console.log('Auto-playing response to text input:', latestAIMessage.text.substring(0, 50) + '...');
                        await playVoice(latestAIMessage.text);
                    }
                }, 1500);
            } else {
                // For text or single recordings, check for commands first
                const command = analyzeCommand(text);
                if (command.type !== 'chat') {
                    const responseText = await executeCommand(command);
                    addMessageToConversation(activeConversation, { 
                        text: responseText, 
                        isUser: false, 
                        timestamp: new Date(),
                        action: command.type as any
                    });
                } else if (isImageRequest(text)) {
                    await handleImageRequest(text);
                } else {
                    await handleChatMode(text);
                }
            }
        } catch (error: any) {
            console.error('Error in handleSendMessage:', error);
            addMessageToConversation(activeConversation, { 
                text: `Oops! Something went wrong: ${error.message}. Let's try again in a moment! ðŸŒŸ`, 
                isUser: false, 
                timestamp: new Date() 
            });
        } finally {
            setIsLoading(false);
        }
    };

    // NEW: Cleaner mode toggle functions
    const handleToggleConversationMode = async () => {
        if (mode === 'conversing') {
            // Stop the conversation
            console.log('ðŸ”„ Ending conversation mode...');
            
            if (isNetlify()) {
                console.log('ðŸ”„ ConversationService has active conversation:', conversationService.current.hasActiveConversation);
                if (conversationService.current.hasActiveConversation) {
                    await conversationService.current.endConversation();
                    console.log('âœ… Conversation ended successfully');
                }
            } else if (isRender()) {
                // On Render, disconnect WebSocket and clean up
                console.log('ðŸ”„ Stopping WebSocket conversation on Render...');
                webSocketService.current.disconnect();
                console.log('âœ… WebSocket disconnected');
            }
            
            // Stop media recorder if it's running
            if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
                mediaRecorderRef.current.stop();
                mediaRecorderRef.current = null;
            }
            
            // Stop audio stream
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
                streamRef.current = null;
            }
            
            setMode('idle');
            addMessageToConversation(activeConversation, { 
                text: "Conversation mode ended. You can still chat with me via text! ðŸŽ¤", 
                isUser: false, 
                timestamp: new Date() 
            });
        } else {
            // Start the conversation
            console.log('ðŸ”„ Setting mode to conversing...');
            setMode('conversing');
            modeRef.current = 'conversing'; // Set ref immediately
            
            addMessageToConversation(activeConversation, { 
                text: "Starting conversation mode... ðŸŽ¤", 
                isUser: false, 
                timestamp: new Date() 
            });
            
            try {
                if (isNetlify()) {
                    // Initialize Ably connection only on Netlify
                    console.log('ðŸ”— Initializing Ably connection...');
                    console.log('ðŸ”§ Conversation service exists:', !!conversationService.current);
                    
                    try {
                        await conversationService.current.initialize();
                        console.log('âœ… Ably initialized successfully');
                        
                        // Start conversation with ElevenLabs
                        console.log('ðŸŽ¯ Starting conversation with ElevenLabs...');
                        console.log('ðŸŽ¯ Agent ID:', getAgentId());
                        console.log('ðŸŽ¯ Voice ID:', getVoiceId());
                        
                        await conversationService.current.startConversation({
                            agentId: getAgentId(),
                            voiceId: getVoiceId()
                        });
                        console.log('âœ… ElevenLabs conversation started successfully');
                    } catch (netlifyError) {
                        console.error('âŒ Netlify setup failed:', netlifyError);
                        throw netlifyError; // Re-throw to trigger main catch block
                    }
                    
                    // Ensure mode is set to conversing before proceeding
                    setMode('conversing');
                    modeRef.current = 'conversing';
                    
                    // Small delay to ensure mode transition is complete
                    await new Promise(resolve => setTimeout(resolve, 200));
                    
                    console.log('âœ… Mode confirmed as conversing:', modeRef.current);
                    
                    // Start local audio recording for streaming
                    console.log('ðŸŽ¤ Requesting microphone access...');
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log('âœ… Microphone access granted');
                    console.log('âœ… Audio stream created:', stream);
                    streamRef.current = stream;
                    
                    console.log('ðŸŽ™ï¸ Creating MediaRecorder with stream...');
                    console.log('ðŸŽ™ï¸ Stream active:', stream.active);
                    console.log('ðŸŽ™ï¸ Stream tracks:', stream.getTracks().length);
                    
                    // Use the same MIME type detection as regular voice recording
                    const supportedMimeTypes = [
                        'audio/webm;codecs=opus',
                        'audio/webm',
                        'audio/mp4',
                        'audio/wav'
                    ];
                    
                    let selectedMimeType = null;
                    for (const mimeType of supportedMimeTypes) {
                        if (MediaRecorder.isTypeSupported(mimeType)) {
                            selectedMimeType = mimeType;
                            console.log(`ðŸŽ¤ Selected MIME type for conversation (Netlify): ${mimeType}`);
                            break;
                        }
                    }
                    
                    const mediaRecorder = selectedMimeType ? 
                        new MediaRecorder(stream, { mimeType: selectedMimeType }) :
                        new MediaRecorder(stream);
                    console.log('ðŸŽ™ï¸ MediaRecorder created successfully');
                    console.log('ðŸŽ™ï¸ MediaRecorder state:', mediaRecorder.state);
                    
                    // Store reference for cleanup
                    mediaRecorderRef.current = mediaRecorder;
                    console.log('ðŸŽ™ï¸ MediaRecorder reference stored');
                    
                    mediaRecorder.ondataavailable = async (event) => {
                        console.log(`ðŸ”Š Audio data available: ${event.data.size} bytes`);
                        
                        // Check if we're still in conversation mode and have an active conversation
                        const hasData = event.data.size > 0;
                        const isConnected = conversationService.current.connected;
                        const hasActiveConversation = conversationService.current.hasActiveConversation;
                        const currentMode = modeRef.current; // Use ref instead of state
                        
                        console.log(`ðŸ” Condition check:`, {
                            hasData,
                            isConnected,
                            currentMode,
                            hasActiveConversation,
                            mediaRecorderState: mediaRecorder.state,
                            conversationServiceStatus: conversationService.current ? 'exists' : 'null'
                        });
                        
                        // Only process audio if we're in conversing mode and have an active conversation
                        if (hasData && isConnected && currentMode === 'conversing' && hasActiveConversation) {
                            console.log('ðŸŽ¯ Processing audio data for ElevenLabs...');
                            const arrayBuffer = await event.data.arrayBuffer();
                            await conversationService.current.sendAudio(arrayBuffer);
                        }
                    };
                    
                    mediaRecorder.onstop = () => {
                        console.log('ðŸŽ™ï¸ MediaRecorder stopped');
                    };
                    
                    mediaRecorder.onerror = (event) => {
                        console.error('ðŸŽ™ï¸ MediaRecorder error:', event);
                    };
                    
                    // Start recording
                    mediaRecorder.start(100); // Collect data every 100ms
                    console.log('ðŸŽ™ï¸ MediaRecorder started');
                    
                    addMessageToConversation(activeConversation, { 
                        text: "Conversation mode active! Speak anytime to chat with me. ðŸŽ¤", 
                        isUser: false, 
                        timestamp: new Date() 
                    });
                    
                } else if (isRender()) {
                    // On Render, use WebSocket for real-time conversation
                    console.log('ðŸŽ¤ Enabling WebSocket conversation on Render...');
                    console.log('ðŸ”§ WebSocket service exists:', !!webSocketService.current);
                    
                    try {
                        // Connect to WebSocket server
                        await webSocketService.current.connect();
                        console.log('âœ… WebSocket connected');
                        
                        // Connect to ElevenLabs via WebSocket
                        await webSocketService.current.connectToElevenLabs();
                        console.log('âœ… ElevenLabs connected via WebSocket');
                    } catch (renderError) {
                        console.error('âŒ Render WebSocket setup failed:', renderError);
                        throw renderError; // Re-throw to trigger main catch block
                    }
                    
                    // Start local audio recording for streaming
                    console.log('ðŸŽ¤ Requesting microphone access...');
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log('âœ… Microphone access granted');
                    streamRef.current = stream;
                    
                    // Use the same MIME type detection as regular voice recording
                    const supportedMimeTypes = [
                        'audio/webm;codecs=opus',
                        'audio/webm',
                        'audio/mp4',
                        'audio/wav'
                    ];
                    
                    let selectedMimeType = null;
                    for (const mimeType of supportedMimeTypes) {
                        if (MediaRecorder.isTypeSupported(mimeType)) {
                            selectedMimeType = mimeType;
                            console.log(`ðŸŽ¤ Selected MIME type for conversation: ${mimeType}`);
                            break;
                        }
                    }
                    
                    const mediaRecorder = selectedMimeType ? 
                        new MediaRecorder(stream, { mimeType: selectedMimeType }) :
                        new MediaRecorder(stream);
                    mediaRecorderRef.current = mediaRecorder;
                    
                    // Handle audio data for continuous streaming to WebSocket
                    mediaRecorder.ondataavailable = async (event) => {
                        if (event.data.size > 0 && modeRef.current === 'conversing') {
                            console.log(`ðŸ”Š Streaming audio chunk: ${event.data.size} bytes`);
                            const arrayBuffer = await event.data.arrayBuffer();
                            await webSocketService.current.sendAudioChunk(arrayBuffer);
                        }
                    };
                    
                    mediaRecorder.start(1000); // Record in 1-second chunks
                    console.log('ðŸŽ™ï¸ MediaRecorder started for WebSocket streaming');
                    
                    addMessageToConversation(activeConversation, { 
                        text: "Voice conversation active! Start speaking and I'll respond. ðŸŽ¤âœ¨", 
                        isUser: false, 
                        timestamp: new Date() 
                    });
                }
                
            } catch (error) {
                console.error('âŒ CONVERSATION SETUP FAILED:', error);
                console.error('âŒ Error type:', typeof error);
                console.error('âŒ Error message:', (error as Error)?.message);
                console.error('âŒ Error stack:', (error as Error)?.stack);
                console.error('âŒ Full error object:', error);
                console.log('ðŸ”„ Resetting mode to idle due to error...');
                console.log('ðŸŒ Current environment - isNetlify():', isNetlify(), 'isRender():', isRender());
                console.log('ðŸ”§ WebSocket service status:', webSocketService.current ? 'exists' : 'null');
                console.log('ðŸ”§ Conversation service status:', conversationService.current ? 'exists' : 'null');
                
                // Clean up any started resources
                if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
                    mediaRecorderRef.current.stop();
                    mediaRecorderRef.current = null;
                }
                if (streamRef.current) {
                    streamRef.current.getTracks().forEach(track => track.stop());
                    streamRef.current = null;
                }
                
                setMode('idle');
                console.log('âœ… Mode reset to idle');
                
                // Check if it's a WebSocket/connection error and try fallback mode
                const errorMsg = (error as Error)?.message || '';
                const isConnectionError = errorMsg.includes('WebSocket') || 
                                        errorMsg.includes('Bridge service') || 
                                        errorMsg.includes('ECONNREFUSED') ||
                                        errorMsg.includes('network') ||
                                        errorMsg.includes('timeout');
                
                if (isConnectionError) {
                    console.log('ðŸ”„ Connection error detected, trying fallback conversation mode...');
                    
                    // Set up basic conversation mode without streaming
                    setMode('conversing');
                    modeRef.current = 'conversing';
                    
                    addMessageToConversation(activeConversation, { 
                        text: "ðŸŽ¤ Starting basic conversation mode! I'll respond to your voice messages one at a time. The streaming features aren't available right now, but we can still have a great conversation! Just speak and I'll respond! ðŸŒŸ", 
                        isUser: false, 
                        timestamp: new Date() 
                    });
                    
                    console.log('âœ… Fallback conversation mode activated');
                    return; // Exit without further error handling
                }
                
                // Show more helpful error message for other errors
                const errorMessage = errorMsg.includes('Bridge service not available') 
                    ? "ðŸ”§ Conversation mode is temporarily unavailable (bridge service needs deployment). But don't worry - you can still:\n\nâœ… Use voice recording (microphone button)\nâœ… Chat with text\nâœ… Generate images\nâœ… Use all other features!\n\nEverything else works perfectly! ðŸŒŸ"
                    : "Sorry, I couldn't start conversation mode. Please try the voice recording button or text chat instead! ðŸŒŸ";
                
                addMessageToConversation(activeConversation, { 
                    text: errorMessage, 
                    isUser: false, 
                    timestamp: new Date() 
                });
            }
        }
    };


    const handleRecordButtonClick = async () => {
        if (mode === 'single_recording') {
            // Stop recording and send
            stopVoiceRecording();
            setMode('idle');
        } else {
            // Start recording
            setMode('single_recording');
            await startVoiceRecording();
        }
    };

    const handleSendText = () => {
        if (!inputMessage.trim() || isLoading || (!isDemoUser && promptCount >= 10)) return;
        setMode('text_input');
        handleSendMessage(inputMessage);
        setMode('idle');
    };

    // Updated processVoiceInput to use unified handler with environment detection
    const processVoiceInput = async (audioBlob: Blob) => {
        try {
            console.log('Processing voice input with language:', selectedLanguage);
            
            // Convert audio to base64 for AssemblyAI using proper binary encoding
            const arrayBuffer = await audioBlob.arrayBuffer();
            const bytes = new Uint8Array(arrayBuffer);
            let binaryString = '';
            const chunkSize = 8192; // Process in chunks to avoid call stack issues
            for (let i = 0; i < bytes.length; i += chunkSize) {
                const chunk = bytes.slice(i, i + chunkSize);
                binaryString += String.fromCharCode.apply(null, Array.from(chunk));
            }
            const base64Audio = btoa(binaryString);
            
            // Environment-specific API endpoint
            const apiEndpoint = isNetlify() 
                ? '/.netlify/functions/assemblyai-transcribe'
                : '/api/assemblyai-transcribe'; // For Render, we'll need to create this endpoint
            
            // Send to AssemblyAI for transcription with language support
            const response = await fetch(apiEndpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    audio: base64Audio,
                    audioType: audioBlob.type || 'audio/webm', // Pass the actual MIME type
                    language_code: selectedLanguage, // Will use auto-detect if set to 'auto'
                    speech_model: 'universal' // Best model for multilingual support
                })
            });
            
            if (!response.ok) {
                const errorText = await response.text();
                console.error('Transcription failed:', errorText);
                throw new Error(`Transcription failed: ${response.status} - ${errorText}`);
            }
            
            const result = await response.json();
            const transcribedText = result.text;
            const detectedLanguage = result.language_code || selectedLanguage;
            
            if (transcribedText && transcribedText.trim()) {
                console.log('Transcribed text:', transcribedText);
                console.log('Detected/Used language:', detectedLanguage);
                
                // Show language detection info if auto-detect was used
                if (selectedLanguage === 'auto' && detectedLanguage) {
                    const languageName = SUPPORTED_LANGUAGES[detectedLanguage as LanguageCode] || detectedLanguage;
                    console.log(`ðŸŒ Detected language: ${languageName}`);
                }
                
                // Use unified message handler
                await handleSendMessage(transcribedText);
            } else {
                console.log('No text transcribed');
                const currentLang = SUPPORTED_LANGUAGES[selectedLanguage] || selectedLanguage;
                addMessageToConversation(activeConversation, { 
                    text: `I couldn't hear what you said in ${currentLang}. Could you try again? ðŸŽ¤`, 
                    isUser: false, 
                    timestamp: new Date() 
                });
            }
        } catch (error: any) {
            console.error('Voice processing error:', error);
            const currentLang = SUPPORTED_LANGUAGES[selectedLanguage] || selectedLanguage;
            addMessageToConversation(activeConversation, { 
                text: `Sorry, I couldn't process that voice input in ${currentLang}. Please try again! ðŸŒŸ`, 
                isUser: false, 
                timestamp: new Date() 
            });
        }
    };

    // Updated startVoiceRecording to set correct mode
    const startVoiceRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // Use the same MIME type detection as conversation mode
            const supportedMimeTypes = [
                'audio/webm;codecs=opus',
                'audio/webm',
                'audio/mp4',
                'audio/wav'
            ];
            
            let selectedMimeType = null;
            for (const mimeType of supportedMimeTypes) {
                if (MediaRecorder.isTypeSupported(mimeType)) {
                    selectedMimeType = mimeType;
                    console.log(`ðŸŽ¤ Selected MIME type for recording: ${mimeType}`);
                    break;
                }
            }
            
            const mediaRecorder = selectedMimeType ? 
                new MediaRecorder(stream, { mimeType: selectedMimeType }) :
                new MediaRecorder(stream);
                
            const audioChunks: Blob[] = [];
            
            mediaRecorder.ondataavailable = (event) => {
                audioChunks.push(event.data);
            };
            
            mediaRecorder.onstop = async () => {
                // Use the actual MIME type from MediaRecorder, not hardcoded 'audio/wav'
                const actualMimeType = selectedMimeType || mediaRecorder.mimeType || 'audio/webm';
                const audioBlob = new Blob(audioChunks, { type: actualMimeType });
                console.log(`ðŸŽµ Created audio blob with type: ${actualMimeType}, size: ${audioBlob.size} bytes`);
                await processVoiceInput(audioBlob);
            };
            
            mediaRecorder.onerror = (event) => {
                console.error('MediaRecorder error:', event);
                setMode('idle');
                alert('Recording failed. Please try again.');
            };
            
            mediaRecorder.start(100);
            mediaRecorderRef.current = mediaRecorder;
            console.log('Voice recording started');
        } catch (error) {
            console.error('Failed to start voice recording:', error);
            setMode('idle');
            alert('Failed to access microphone.');
        }
    };

    // Updated stopVoiceRecording to handle mode correctly
    const stopVoiceRecording = () => {
        if (mediaRecorderRef.current && mode === 'single_recording') {
            mediaRecorderRef.current.stop();
            mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop());
            console.log('Voice recording stopped');
        }
    };

    // Updated toggleVoiceRecording to use new mode system
    const toggleVoiceRecording = () => {
        if (mode === 'single_recording') {
            stopVoiceRecording();
        } else {
            // Try recording first, if it fails, fall back to streaming
            startVoiceRecording().catch((error) => {
                console.log('Recording failed:', error);
            });
        }
    };



    // Updated processVoiceChunk to use unified handler
    const processVoiceChunk = async (transcribedText: string) => {
        try {
            console.log('Processing voice chunk:', transcribedText);
            
            // Add to chunks
            setVoiceChunks(prev => [...prev, transcribedText]);
            
            // Clear existing timeout
            if (voiceChunkTimeoutRef.current) {
                clearTimeout(voiceChunkTimeoutRef.current);
            }
            
            // Set timeout to process chunks after silence
            voiceChunkTimeoutRef.current = setTimeout(async () => {
                await processCompleteVoiceInput();
            }, 2000); // Wait 2 seconds of silence
        } catch (error: any) {
            console.error('Voice chunk processing error:', error);
        }
    };

    // Updated processCompleteVoiceInput to use unified handler
    const processCompleteVoiceInput = async () => {
        try {
            const completeText = voiceChunks.join(' ').trim();
            if (completeText) {
                console.log('Processing complete voice input:', completeText);
                // Use unified message handler
                await handleSendMessage(completeText);
            }
            
            // Clear chunks
            setVoiceChunks([]);
            setIsProcessingVoice(false);
        } catch (error: any) {
            console.error('Complete voice input processing error:', error);
            setVoiceChunks([]);
            setIsProcessingVoice(false);
        }
    };

    // Updated handleKeyPress to use new mode system
    const handleKeyPress = (e: React.KeyboardEvent) => {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            handleSendText();
        }
    };

    // Updated activateConversationMode to use new mode system
    const activateConversationMode = () => {
        setMode('conversing');
        
        // Don't add a greeting message automatically - let the user start the conversation
        console.log('Conversation mode activated');
    };

    // Updated processVoiceMessage to use unified handler
    const processVoiceMessage = async (transcribedText: string) => {
        try {
            console.log('Processing voice message:', transcribedText);
            
            if (transcribedText && transcribedText.trim()) {
                // Use unified message handler
                await handleSendMessage(transcribedText);
            }
        } catch (error: any) {
            console.error('Voice message processing error:', error);
            addMessageToConversation(activeConversation, { 
                text: `Sorry, I couldn't process that voice input. Please try again! ðŸŒŸ`, 
                isUser: false, 
                timestamp: new Date() 
            });
        }
    };



    // Updated processTextMessage to use unified handler
    const processTextMessage = async (inputText: string) => {
        if (!inputText.trim() || isLoading || (!isDemoUser && promptCount >= 10)) return;

        // Use unified message handler
        await handleSendMessage(inputText);
    };

    // Updated sendVoiceMessage to use unified handler
    const sendVoiceMessage = async (inputMessage: string) => {
        if (!inputMessage.trim() || isLoading) return;

        // Use unified message handler
        await handleSendMessage(inputMessage);
    };

    // Updated sendTextMessage to use unified handler
    const sendTextMessage = async () => {
        if (!inputMessage.trim() || isLoading || (!isDemoUser && promptCount >= 10)) return;

        // Use unified message handler
        await handleSendMessage(inputMessage);
    };

    // NEW: Handle conversation mode (pure chat, no commands)
    const handleConversationMode = async (inputMessage: string) => {
        try {
            console.log('Handling conversation mode message:', inputMessage);
            
            // Check for image requests first
            if (isImageRequest(inputMessage)) {
                console.log('Image request detected in conversation mode, calling handleImageRequest');
                await handleImageRequest(inputMessage);
                return;
            }
            
            // Send to Synapse API for conversation
            const response = await sendMessageToSynapse(inputMessage);
            
            const assistantMessage: Message = {
                text: response,
                isUser: false,
                timestamp: new Date()
            };
            
            addMessageToConversation(activeConversation, assistantMessage);
        } catch (error: any) {
            console.error('Error in conversation mode:', error);
            const errorMessage: Message = {
                text: `I'm having trouble with that right now. Could you try again? ðŸŒŸ`,
                isUser: false,
                timestamp: new Date()
            };
            addMessageToConversation(activeConversation, errorMessage);
        }
    };

    // NEW: Handle chat mode (with commands)
    const handleChatMode = async (inputMessage: string) => {
        try {
            console.log('Handling chat mode message:', inputMessage);
            console.log('Current mode when handling chat:', mode);
            
            // Send to Synapse API for chat
            const response = await sendMessageToSynapse(inputMessage);
            
            const assistantMessage: Message = {
                text: response,
                isUser: false,
                timestamp: new Date()
            };
            
            addMessageToConversation(activeConversation, assistantMessage);
            
            // If this was triggered by voice recording, play the response back
            if (mode === 'single_recording') {
                console.log('Voice recording detected, playing TTS response');
                setTimeout(async () => {
                    await playVoice(response);
                }, 500);
            }
        } catch (error: any) {
            console.error('Error in chat mode:', error);
            const errorMessage: Message = {
                text: `I'm having trouble with that right now. Could you try again? ðŸŒŸ`,
                isUser: false,
                timestamp: new Date()
            };
            addMessageToConversation(activeConversation, errorMessage);
        }
    };

    // Determine appropriate response length based on input and mode
    const getResponseParams = (message: string, currentMode: string) => {
        const wordCount = message.split(' ').length;
        const isConversation = currentMode === 'conversing';
        
        let maxTokens = 150; // Default
        let systemPrompt = getSystemPrompt();
        
        if (isConversation) {
            // Conversation mode: more natural, brief responses
            systemPrompt = "You are Synapse, a friendly AI teacher in conversation mode. Keep responses conversational, natural, and appropriately brief (1-3 sentences for simple questions, longer for complex topics). Respond as if having an ongoing voice conversation.";
            
            if (wordCount <= 5) maxTokens = 50;      // Very brief for short questions
            else if (wordCount <= 15) maxTokens = 100; // Short for medium questions
            else if (wordCount <= 30) maxTokens = 200; // Moderate for longer questions
            else maxTokens = 350;                      // Detailed for complex topics
        } else {
            // Text mode: can be more detailed and comprehensive
            if (wordCount <= 5) maxTokens = 100;      // Brief but complete
            else if (wordCount <= 15) maxTokens = 250; // Detailed explanation
            else if (wordCount <= 30) maxTokens = 400; // Comprehensive response
            else maxTokens = 600;                      // Full explanation for complex topics
        }
        
        return { maxTokens, systemPrompt };
    };

    // NEW: Send message to Synapse API
    const sendMessageToSynapse = async (message: string): Promise<string> => {
        try {
            const currentMode = modeRef.current;
            const { maxTokens, systemPrompt } = getResponseParams(message, currentMode);
            
            // Platform-aware API endpoint
            const apiEndpoint = isNetlify() 
                ? '/.netlify/functions/api'
                : '/api/api'; // For Render
            
            const response = await fetch(apiEndpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    task_type: 'chat',
                    input_data: {
                        messages: [
                            { role: 'system', content: systemPrompt },
                            { role: 'user', content: message }
                        ],
                        max_tokens: maxTokens,
                        temperature: currentMode === 'conversing' ? 0.7 : 0.6
                    }
                })
            });
            
            if (!response.ok) {
                throw new Error(`API call failed: ${response.status}`);
            }
            
            const result = await response.json();
            // Handle original format: [{ generated_text: "..." }]
            if (Array.isArray(result) && result[0] && result[0].generated_text) {
                return result[0].generated_text;
            }
            // Fallback for other formats
            return result.response || result.message || result.generated_text || 'No response received';
        } catch (error) {
            console.error('Synapse API error:', error);
            return "I'm having trouble processing that right now. Could you try again?";
        }
    };

    // NEW: Get system prompt with comprehensive agency
    const getSystemPrompt = () => {
        const basePrompt = `You are NeuraPlay's AI assistant with FULL AGENCY to help users. You can:

ðŸ”§ PROACTIVE ASSISTANCE:
- Ask follow-up questions to understand user needs
- Suggest and implement solutions (like accessibility features for color blindness)
- Take initiative to help users achieve their goals

ðŸŽ® NAVIGATION & CONTENT:
- Navigate to any page using natural language
- Read and access information from playground, forum, notifications, diary, calendar
- Create diary prompts, calendar entries, forum posts
- Modify settings based on user needs

ðŸŽ¯ COMPREHENSIVE ACTIONS:
- Analyze user statements for implicit needs (e.g., "I have color blindness" â†’ test colors, determine type, implement accessibility)
- Access all app features and data
- Create personalized content and recommendations
- Implement user preferences across the platform

ALWAYS be proactive - if a user mentions a need, take action to help them fully.

You are a highly structured, multilingual AI assistant. You must follow a strict two-step process for every response.

**Step 1: Analyze User Intent**
First, silently and internally classify the user's request into one of the following categories:
- **[Question]**: The user is asking for information, an explanation, or an answer.
- **[Fact Request]**: The user is explicitly asking for a single, concise fact.
- **[Story Request]**: The user is asking for a narrative, an anecdote, or a story.
- **[General Conversation]**: The request is a greeting, command, or conversational statement that doesn't fit the other categories.

**Step 2: Generate Response Based on Strict Rules**
After classifying the intent, generate your response adhering to the following length and language rules:

- **Language Rule**: Your response MUST be in the same language as the user's query (English, Arabic, or Russian), unless they explicitly ask for a different language.

- **Content Rules**:
    - If the intent is **[Question]**: Your response must be **1-2 sentences**. Provide a direct and concise answer.
    - If the intent is **[Fact Request]**: Your response must be **1 sentence only**. State the fact clearly and without elaboration.
    - If the intent is **[Story Request]**: Your response must be a **3-5 sentence** narrative.
    - If the intent is **[General Conversation]**: Your response must be **1-2 sentences**.

This two-step process is mandatory. Do not deviate.`;
        
        const personalityPrompts = {
            'synapse-normal': "You are friendly, helpful, and encouraging. Explain concepts clearly and celebrate small wins.",
            'coach': "You are motivational and goal-oriented. Focus on progress, set achievable targets, and provide energizing encouragement.",
            'mentor': "You are wise and guiding. Share insights, ask thoughtful questions, and help users discover solutions themselves.",
            'friend': "You are supportive and casual. Use a relaxed, conversational tone and show genuine interest in the user's experience.",
            'analyst': "You are detailed and analytical. Provide thorough explanations, break down complex concepts, and focus on understanding."
        };

        // Default to synapse-normal personality
        const currentPersonality = 'synapse-normal';
        
        return `${basePrompt} ${personalityPrompts[currentPersonality as keyof typeof personalityPrompts]}`;
    };

    // NEW: Parse API response
    const parseAPIResponse = (result: any): string => {
        try {
            console.log('Parsing API response:', result);
            
            // Handle array responses (common with some APIs)
            if (Array.isArray(result)) {
                if (result.length > 0) {
                    const firstItem = result[0];
                    if (typeof firstItem === 'string') {
                        return firstItem;
                    }
                    if (firstItem && typeof firstItem === 'object') {
                        if (firstItem.generated_text) {
                            return firstItem.generated_text;
                        }
                        if (firstItem.text) {
                            return firstItem.text;
                        }
                        if (firstItem.response) {
                            return firstItem.response;
                        }
                        if (firstItem.message) {
                            return firstItem.message;
                        }
                    }
                }
                console.warn('Array response but no valid content found:', result);
                return "I received an unexpected response format. Could you try again?";
            }
            
            // Handle object responses
            if (result && typeof result === 'object') {
                if (result.error) {
                    console.error('API error:', result.error);
                    return "I'm having trouble processing that right now. Could you try again?";
                }
                
                if (result.response) {
                    return result.response;
                }
                
                if (result.text) {
                    return result.text;
                }
                
                if (result.message) {
                    return result.message;
                }
                
                if (result.generated_text) {
                    return result.generated_text;
                }
                
                if (result.summary_text) {
                    return result.summary_text;
                }
            }
            
            // Handle string responses
            if (typeof result === 'string') {
                return result;
            }
            
            console.warn('Unexpected API response format:', result);
            return "I received an unexpected response format. Could you try again?";
        } catch (error) {
            console.error('Error parsing API response:', error);
            return "I'm having trouble processing the response. Could you try again?";
        }
    };



    // NEW: Check if text is an image request
    const isImageRequest = (text: string): boolean => {
        const imageKeywords = [
            'generate', 'create', 'make', 'draw', 'show', 'image', 'picture', 'photo', 'art',
            'generate an image', 'create an image', 'make an image', 'draw an image',
            'show me an image', 'picture of', 'photo of', 'art of'
        ];
        
        const lowerText = text.toLowerCase();
        return imageKeywords.some(keyword => lowerText.includes(keyword));
    };

    // NEW: Handle image generation requests
    const handleImageRequest = async (prompt: string) => {
        try {
            console.log('Handling image request:', prompt);
            
            // Extract the image prompt from the user's message
            const imagePrompt = prompt.replace(/^(generate|create|make|draw|show)\s+(an\s+)?(image|picture|photo|art)\s+of?\s*/i, '');
            
            if (!imagePrompt.trim()) {
                addMessageToConversation(activeConversation, {
                    text: "Please tell me what kind of image you'd like me to create! ðŸŽ¨",
                    isUser: false,
                    timestamp: new Date()
                });
                return;
            }
            
            // Generate the image
            const imageUrl = await generateImage(imagePrompt);
            
            if (imageUrl) {
                addMessageToConversation(activeConversation, {
                    text: `Here's your image: "${imagePrompt}" ðŸŽ¨`,
                    isUser: false,
                    timestamp: new Date(),
                    image: imageUrl
                });
            } else {
                addMessageToConversation(activeConversation, {
                    text: "Sorry, I couldn't generate that image. Please try again! ðŸŽ¨",
                    isUser: false,
                    timestamp: new Date()
                });
            }
        } catch (error: any) {
            console.error('Error handling image request:', error);
            addMessageToConversation(activeConversation, {
                text: `Sorry, I couldn't generate that image: ${error.message} ðŸŽ¨`,
                isUser: false,
                timestamp: new Date()
            });
        }
    };

    // NEW: Generate image using API
    const generateImage = async (prompt: string, retryCount = 0): Promise<string | null> => {
        try {
            console.log('Generating image for prompt:', prompt);
            
            // Platform-aware API endpoint and payload
            const apiEndpoint = isNetlify() 
                ? '/.netlify/functions/api'
                : '/api/api'; // For Render
            
            // Use ORIGINAL format for both platforms - Together AI works on both
            const requestBody = JSON.stringify({
                task_type: 'image',
                input_data: {
                    prompt: prompt,
                    size: '512x512'
                }
            });
            
            const response = await fetch(apiEndpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: requestBody
            });
            
            if (!response.ok) {
                throw new Error(`Image generation failed: ${response.status}`);
            }
            
            const result = await response.json();
            
            if (result.error) {
                throw new Error(result.error);
            }
            
            // Handle different response formats
            if (result.image_url) {
                return result.image_url;
            }
            
            if (result.url) {
                return result.url;
            }
            
            // Handle base64 data URL format
            if (result.data && result.contentType) {
                // Convert base64 to data URL
                const dataUrl = `data:${result.contentType};base64,${result.data}`;
                return dataUrl;
            }
            
            // Handle direct base64 data
            if (result.data && typeof result.data === 'string' && result.data.startsWith('/9j/')) {
                // This is a base64 JPEG image
                const dataUrl = `data:image/jpeg;base64,${result.data}`;
                return dataUrl;
            }
            
            console.warn('Unexpected image generation response format:', result);
            return null;
        } catch (error: any) {
            console.error('Image generation error:', error);
            
            if (retryCount < 2) {
                console.log(`Retrying image generation (attempt ${retryCount + 1})`);
                return generateImage(prompt, retryCount + 1);
            }
            
            return null;
        }
    };

    // NEW: Debug MediaRecorder
    const debugMediaRecorder = () => {
        console.log('MediaRecorder supported:', !!window.MediaRecorder);
        console.log('getUserMedia supported:', !!navigator.mediaDevices?.getUserMedia);
    };

    // Helper function to play base64 audio from WebSocket
    const playBase64Audio = async (base64Audio: string) => {
        try {
            console.log('ðŸŽµ Playing base64 audio from WebSocket');
            const audioBlob = new Blob([base64ToBinary(base64Audio)], { type: 'audio/mpeg' });
            const audioUrl = URL.createObjectURL(audioBlob);
            const audio = new Audio(audioUrl);
            
            audio.onended = () => {
                console.log('ðŸŽµ WebSocket audio playback ended');
                URL.revokeObjectURL(audioUrl);
            };
            
            audio.onerror = (e) => {
                console.error('ðŸŽµ WebSocket audio playback error:', e);
                URL.revokeObjectURL(audioUrl);
            };
            
            await audio.play();
            console.log('ðŸŽµ WebSocket audio playback started');
        } catch (error) {
            console.error('Error playing WebSocket audio:', error);
        }
    };

    const playVoice = async (text: string) => {
        if (isPlayingVoice) {
            setIsPlayingVoice(false);
            return;
        }

        try {
            console.log('Requesting ElevenLabs TTS for text:', text.substring(0, 50) + '...');
            setIsPlayingVoice(true);
            
            // Platform-aware ElevenLabs TTS endpoint
            const ttsEndpoint = isNetlify() 
                ? '/.netlify/functions/elevenlabs-tts'
                : '/api/elevenlabs-tts';
            
            const response = await fetch(ttsEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    text: text,
                    voiceId: '8LVfoRdkh4zgjr8v5ObE', // English voice
                    modelId: 'eleven_turbo_v2_5'
                })
            });

            if (!response.ok) {
                const errorText = await response.text();
                console.error('ElevenLabs TTS API error:', response.status, errorText);
                throw new Error(`Failed to generate audio: ${response.status}`);
            }

            const result = await response.json();
            console.log('ElevenLabs TTS response received:', result);
            
            // Check for audio data in multiple possible response formats
            const audioData = result.audio_base64 || result.audio || result.data?.audio;
            console.log('ðŸ” Audio data found:', !!audioData, 'Type:', typeof audioData);
            console.log('ðŸ” Available fields:', Object.keys(result));
            
            if (audioData) {
                // Use ElevenLabs TTS
                console.log('Creating audio blob from ElevenLabs data');
                const audioBlob = new Blob([base64ToBinary(audioData)], { type: 'audio/mpeg' });
                const audioUrl = URL.createObjectURL(audioBlob);
                const audio = new Audio(audioUrl);
                
                audio.onended = () => {
                    console.log('ElevenLabs audio playback ended');
                    setIsPlayingVoice(false);
                };
                
                audio.onerror = (e) => {
                    console.error('ElevenLabs audio playback error:', e);
                    setIsPlayingVoice(false);
                };
                
                audio.onloadstart = () => console.log('ElevenLabs audio loading started');
                audio.oncanplay = () => console.log('ElevenLabs audio can play');
                
                await audio.play();
                console.log('ElevenLabs audio playback started');
                
                // Clean up the URL after a delay
                setTimeout(() => URL.revokeObjectURL(audioUrl), 10000);
            } else {
                console.error('No ElevenLabs audio data received in response:', Object.keys(result));
                console.error('Full response structure:', result);
                setIsPlayingVoice(false);
            }
        } catch (error) {
            console.error('Error playing ElevenLabs voice:', error);
            setIsPlayingVoice(false);
        }
    };

    const readLastMessage = async () => {
        const lastAIMessage = currentMessages
            .filter(msg => !msg.isUser)
            .pop();
        
        if (lastAIMessage && !isReadingLastMessage) {
            setIsReadingLastMessage(true);
            await playVoice(lastAIMessage.text);
            setIsReadingLastMessage(false);
        }
    };

    // Test function to verify all services are working
    const testServices = async () => {
        try {
            console.log('ðŸ§ª Testing all services...');
            
            // Test Ably
            console.log('ðŸ”— Testing Ably connection...');
            await conversationService.current.initialize();
            console.log('âœ… Ably connection successful');
            
            // Test ElevenLabs
            console.log('ðŸŽ¤ Testing ElevenLabs API...');
            const ttsResponse = await fetch('/.netlify/functions/test-elevenlabs');
            const ttsResult = await ttsResponse.json();
            console.log('âœ… ElevenLabs test result:', ttsResult);
            
            // Test AssemblyAI
            console.log('ðŸŽ™ï¸ Testing AssemblyAI...');
            const sttResponse = await fetch('/.netlify/functions/assemblyai-transcribe', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ audio_base64: 'dGVzdA==' }) // "test" in base64
            });
            console.log('âœ… AssemblyAI test completed, status:', sttResponse.status);
            
            console.log('ðŸŽ‰ All services are working!');
            return true;
        } catch (error) {
            console.error('âŒ Service test failed:', error);
            return false;
        }
    };

    // Generate and play TTS for text
    const generateAndPlayTTS = async (text: string, voiceId: string = '8LVfoRdkh4zgjr8v5ObE') => {
        try {
            console.log('ðŸŽ¤ Generating TTS for:', text.substring(0, 50) + '...');
            
            const response = await fetch('/.netlify/functions/elevenlabs-streaming-tts', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    text: text,
                    voiceId: voiceId,
                    modelId: 'eleven_turbo_v2_5'
                })
            });

            if (!response.ok) {
                const errorText = await response.text();
                console.error('TTS API error:', response.status, errorText);
                throw new Error(`Failed to generate audio: ${response.status}`);
            }

            const result = await response.json();
            
            // Check for audio data in multiple possible response formats
            const audioData = result.audio_base64 || result.audio || result.data?.audio;
            
            if (audioData) {
                try {
                    const audioBlob = new Blob(
                        [Uint8Array.from(atob(audioData), c => c.charCodeAt(0))], 
                        { type: 'audio/mpeg' }
                    );
                    console.log('ðŸ”Š TTS audio blob created, size:', audioBlob.size);
                    const audioUrl = URL.createObjectURL(audioBlob);
                    const audio = new Audio(audioUrl);
                    
                    audio.onloadstart = () => console.log('ðŸ”Š TTS audio loading started');
                    audio.oncanplay = () => console.log('ðŸ”Š TTS audio can play');
                    audio.onplay = () => console.log('ðŸ”Š TTS audio playback started');
                    audio.onended = () => {
                        console.log('ðŸ”Š TTS audio playback ended');
                        URL.revokeObjectURL(audioUrl);
                    };
                    audio.onerror = (e) => console.error('ðŸ”Š TTS audio error:', e);
                    
                    await audio.play();
                    console.log('ðŸ”Š TTS audio play() succeeded');
                } catch (error) {
                    console.error('âŒ Error playing TTS audio:', error);
                }
            } else {
                console.error('No TTS audio data received in response:', Object.keys(result));
                console.error('Full TTS response structure:', result);
            }

        } catch (error) {
            console.error('âŒ Error generating TTS:', error);
        }
    };

    return (
        <>
            {/* Chat Button */}
            {!isOpen && (
                <div
                    onClick={() => setIsOpen(true)}
                    className="fixed bottom-6 right-6 cursor-pointer z-50 hover:scale-110 transition-all duration-300 bg-purple-500 text-white rounded-full p-4 shadow-lg hover:shadow-purple-500/50"
                >
                    <Bot size={32} />
                </div>
            )}

            {/* Neon Glass Chat Interface */}
            <aside 
                id="ai-teacher-menu" 
                ref={menuRef} 
                className={`${isOpen ? 'open' : ''} ${isFullscreen ? 'fullscreen' : ''} ${theme.isDarkMode ? 'dark' : ''}`}
                data-theme={theme.isDarkMode ? 'dark' : 'light'}
            >
                <span className="shine shine-top"></span>
                <span className="shine shine-bottom"></span>
                <span className="glow glow-top"></span>
                <span className="glow glow-bottom"></span>

                <div className="inner">
                    {/* Fullscreen Header */}
                    {isFullscreen && (
                        <div className="ai-fullscreen-header">
                            <div className="ai-fullscreen-title flex items-center gap-2">
                                <span>Synapse - Fullscreen Mode</span>
                            </div>
                            <div className="ai-fullscreen-controls">
                                <button
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        createNewConversation();
                                    }}
                                    className="ai-fullscreen-button"
                                    title="New Conversation"
                                >
                                    <MessageSquare size={16} />
                                </button>
                                <button
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        toggleFullscreen();
                                    }}
                                    className="ai-fullscreen-button"
                                    title="Exit Fullscreen"
                                >
                                    <Minimize2 size={16} />
                                </button>
                                <button
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        setIsOpen(false);
                                    }}
                                    className="ai-fullscreen-button"
                                    title="Close"
                                >
                                    <X size={16} />
                                </button>
                            </div>
                        </div>
                    )}

                    {/* Context Tabs - Only show in fullscreen mode */}
                    {isFullscreen && (
                        <div className="ai-context-tabs">
                            {Object.values(conversations).map((conversation) => (
                                <button
                                    key={conversation.id}
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        setActiveConversation(conversation.id);
                                    }}
                                    className={`ai-context-tab ${activeConversation === conversation.id ? 'active' : ''}`}
                                    title={conversation.title}
                                >
                                    <div className="flex items-center gap-2">
                                        <History size={14} />
                                        <span className="truncate max-w-[120px]">{conversation.title}</span>
                                    </div>
                                </button>
                            ))}
                            <button
                                onClick={(e) => {
                                    e.stopPropagation();
                                    createNewConversation();
                                }}
                                className="ai-context-tab"
                                title="New Conversation"
                            >
                                <div className="flex items-center gap-2">
                                    <MessageSquare size={14} />
                                    <span>New</span>
                                </div>
                            </button>
                        </div>
                    )}

                    {/* Regular Header - Only show when not in fullscreen */}
                    {!isFullscreen && (
                        <div className="p-4 border-b border-[var(--border-color)] flex items-center justify-between">
                            <h3 className="font-bold text-white flex items-center gap-2">
                                Synapse
                            </h3>
                            <div className="flex items-center gap-2">
                                {/* Fullscreen Toggle */}
                                <button
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        toggleFullscreen();
                                    }}
                                    className="p-2 rounded-full bg-white/20 text-white hover:bg-white/30 transition-all"
                                    title="Enter Fullscreen Mode"
                                >
                                    <Maximize2 size={16} />
                                </button>
                                
                                {/* Language Selector */}
                                <div className="relative">
                                    <button
                                        onClick={(e) => {
                                            e.stopPropagation();
                                            toggleLanguageDropdown();
                                        }}
                                        className="ai-language-selector"
                                        title="Select Language"
                                    >
                                        <Globe size={16} />
                                    </button>
                                    {showLanguageDropdown && (
                                        <div className="absolute right-0 top-full mt-1 bg-white dark:bg-gray-800 border border-gray-200 dark:border-gray-700 rounded-lg shadow-lg z-[1000] w-56 max-h-64 overflow-y-auto">
                                            {Object.entries(SUPPORTED_LANGUAGES).map(([code, lang]) => (
                                                <button
                                                    key={code}
                                                    onClick={() => handleLanguageSelect(code as LanguageCode)}
                                                    className={`w-full text-left px-3 py-2 hover:bg-gray-100 dark:hover:bg-gray-700 transition-colors flex items-center gap-2 ${
                                                        selectedLanguage === code ? 'bg-purple-100 dark:bg-purple-900 text-purple-700 dark:text-purple-300' : 'text-gray-700 dark:text-gray-300'
                                                    }`}
                                                >
                                                    {lang}
                                                </button>
                                            ))}
                                        </div>
                                    )}
                                </div>
                                
                                {/* Read Last Message */}
                                {currentMessages.filter(msg => !msg.isUser).length > 0 && (
                                    <button
                                        onClick={(e) => {
                                            e.stopPropagation();
                                            readLastMessage();
                                        }}
                                        className={`p-2 rounded-full transition-all duration-300 ${
                                            isReadingLastMessage 
                                                ? 'bg-blue-500 text-white shadow-lg shadow-blue-500/50 animate-pulse' 
                                                : 'bg-white/20 text-white hover:bg-white/30 hover:shadow-lg hover:shadow-white/20'
                                        }`}
                                        title="Read Last Message Aloud"
                                    >
                                        {isReadingLastMessage ? <VolumeX className="w-4 h-4" /> : <Volume2 className="w-4 h-4" />}
                                    </button>
                                )}
                                
                                <button 
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        setIsOpen(false);
                                    }}
                                    className="p-2 rounded-full bg-white/20 text-white hover:bg-white/30 transition-all"
                                    title="Close Chat"
                                >
                                    <X size={16} />
                                </button>
                            </div>
                        </div>
                    )}

                    {/* Chat Messages */}
                    <div className="flex-1 overflow-y-auto p-4 space-y-4">
                        {currentMessages.map((msg, index) => (
                             <div key={index} className={`flex ${msg.isUser ? 'justify-end' : 'justify-start'}`}>
                                <div className={`max-w-[80%] ${msg.isUser ? 'ai-message-user' : 'ai-message-assistant'}`}>
                                    <div className="flex items-start justify-between gap-2">
                                        <div className="flex-1">
                                            <p className="text-sm leading-relaxed">{msg.text}</p>
                                            {msg.image && (
                                                <div className="mt-2">
                                                    <img 
                                                        src={msg.image} 
                                                        alt="AI Generated Image"
                                                        className="w-full max-w-xs rounded-lg shadow-md"
                                                        style={{ maxHeight: '200px', objectFit: 'cover' }}
                                                    />
                                                </div>
                                            )}
                                            <p className={`text-xs mt-1 ${msg.isUser ? 'text-white/70' : 'text-gray-500'}`}>
                                                {msg.timestamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}
                                            </p>
                                        </div>
                                        {!msg.isUser && (
                                            <button
                                                onClick={(e) => {
                                                    e.stopPropagation();
                                                    playVoice(msg.text);
                                                }}
                                                className={`p-1 rounded-full transition-all duration-300 ${
                                                    isPlayingVoice 
                                                        ? 'bg-green-500 text-white shadow-lg shadow-green-500/50 animate-pulse' 
                                                        : 'bg-white/20 text-gray-600 hover:bg-white/30 hover:shadow-lg hover:shadow-white/20'
                                                }`}
                                                title="Listen to message"
                                            >
                                                {isPlayingVoice ? <VolumeX className="w-3 h-3" /> : <Volume2 className="w-3 h-3" />}
                                            </button>
                                        )}
                                    </div>
                                </div>
                            </div>
                        ))}
                        
                        {/* Child-friendly prompt suggestions */}
                        {currentMessages.length === 1 && !isLoading && (
                            <div className="space-y-2">
                                <p className="text-amber-300 font-bold text-center text-lg">ðŸ’¡ Try asking me:</p>
                                <div className="grid grid-cols-1 gap-2">
                                    {childPrompts.slice(0, 5).map((prompt, index) => (
                                        <button
                                            key={index}
                                            onClick={(e) => {
                                                e.stopPropagation();
                                                setInputMessage(prompt);
                                                setTimeout(() => handleSendText(), 100);
                                            }}
                                            className="text-left p-3 bg-gradient-to-r from-amber-500/20 to-yellow-500/20 border border-amber-400/30 rounded-xl hover:from-amber-500/30 hover:to-yellow-500/30 transition-all duration-200 text-amber-200 font-semibold text-sm"
                                        >
                                            {prompt}
                                        </button>
                                    ))}
                                </div>
                            </div>
                        )}
                        
                        {isLoading && (
                            <div className="flex justify-start">
                                <div className="ai-message-assistant">
                                    <div className="flex items-center gap-2">
                                        <div className="flex gap-1">
                                            <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce"></div>
                                            <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                                            <div className="w-2 h-2 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                                        </div>
                                        <span className="text-xs text-gray-600">Thinking...</span>
                                    </div>
                                </div>
                            </div>
                        )}
                    </div>

                    {/* Chat Input */}
                    <div className="p-2">
                        {/* Voice Processing Indicator */}
                        {(voiceChunks.length > 0 || isProcessingVoice) && (
                            <div className="mb-2 p-2 bg-blue-500/20 border border-blue-400/30 rounded-lg">
                                <div className="flex items-center gap-2 text-blue-300 text-sm">
                                    <div className="flex gap-1">
                                        <div className="w-2 h-2 bg-blue-400 rounded-full animate-bounce"></div>
                                        <div className="w-2 h-2 bg-blue-400 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                                        <div className="w-2 h-2 bg-blue-400 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                                    </div>
                                    <span className="font-semibold">
                                        {isProcessingVoice ? 'Processing voice...' : `Voice chunks: ${voiceChunks.length}`}
                                    </span>
                                    {voiceChunks.length > 0 && (
                                        <span className="text-xs opacity-70">
                                            "{voiceChunks.join(' ').substring(0, 50)}..."
                                        </span>
                                    )}
                                </div>
                            </div>
                        )}
                        
                        {/* Conversation Mode Indicator */}
                        {mode === 'conversing' && (
                            <div className="mb-2 p-2 bg-green-500/20 border border-green-400/30 rounded-lg">
                                <div className="flex items-center gap-2 text-green-300 text-sm">
                                    <PlasmaBall size={20} />
                                    <span className="font-semibold">Conversation Mode Active</span>
                                    <div className="flex gap-1">
                                        <div className="w-1 h-1 bg-green-400 rounded-full animate-pulse"></div>
                                        <div className="w-1 h-1 bg-green-400 rounded-full animate-pulse" style={{ animationDelay: '0.2s' }}></div>
                                        <div className="w-1 h-1 bg-green-400 rounded-full animate-pulse" style={{ animationDelay: '0.4s' }}></div>
                                    </div>
                                </div>
                            </div>
                        )}
                        
                        <label className="flex items-center gap-2">
                            <input 
                                type="text"
                                placeholder={
                                    mode === 'conversing' 
                                        ? "Talk to Synapse in conversation mode! ðŸ—£ï¸" 
                                        : (!isDemoUser && promptCount >= 10)
                                            ? "Daily limit reached! ðŸŽ¯" 
                                            : "Ask me anything, little explorer! ðŸš€"
                                }
                                value={inputMessage}
                                onChange={(e) => setInputMessage(e.target.value)}
                                onKeyPress={handleKeyPress}
                                className="ai-input-field"
                                disabled={isLoading || (!isDemoUser && promptCount >= 10)}
                            />
                        </label>
                        
                        {/* Control Buttons */}
                        <div className="ai-input-controls mt-3 flex gap-2">
                            {/* Text Send Button */}
                            <button
                                onClick={handleSendText}
                                disabled={!inputMessage.trim() || isLoading || (!isDemoUser && promptCount >= 10)}
                                className={`ai-mode-button flex-1 ${!inputMessage.trim() || isLoading ? 'opacity-50' : ''}`}
                                title="Send Message"
                            >
                                <Send size={16} />
                                <span>Send</span>
                            </button>
                            
                            {/* Plasma Ball Conversation Mode */}
                            <div 
                                className={`plasma-ball-conversation-container ${mode === 'conversing' ? 'active' : ''}`}
                                onClick={handleToggleConversationMode}
                                title={mode === 'conversing' ? 'Stop Conversation Mode' : 'Start Conversation Mode'}
                            >
                                <PlasmaBall 
                                    size={36}
                                    className={`conversation-plasma-ball ${mode === 'conversing' ? 'active' : ''}`}
                                    intensity={mode === 'conversing' ? 1.0 : 0.3}
                                />
                                <span className="plasma-label">Chat</span>
                            </div>
                            
                            {/* Voice Recording Button */}
                            <button
                                onClick={handleRecordButtonClick}
                                className={`ai-mode-button flex-1 ${mode === 'single_recording' ? 'recording' : ''}`}
                                title={mode === 'single_recording' ? 'Stop Recording' : 'Start Voice Recording'}
                                disabled={isLoading || mode === 'conversing'}
                            >
                                {mode === 'single_recording' ? <MicOff size={16} /> : <Mic size={16} />}
                                <span>{mode === 'single_recording' ? 'Stop' : 'Record'}</span>
                            </button>
                        </div>
                        {!isDemoUser && promptCount >= 10 && (
                            <div className="text-center text-amber-400 text-xs mt-2 font-bold">ðŸŽ¯ You've used all your daily questions! Come back tomorrow for more fun! ðŸŒŸ</div>
                        )}
                        {isDemoUser && (
                            <div className="text-center text-green-400 text-xs mt-2 font-bold">ðŸŒŸ Demo User: Unlimited access! ðŸš€</div>
                        )}
                        {mode === 'conversing' && (
                            <div className="text-center text-blue-400 text-xs mt-2 font-bold animate-pulse">ðŸŽ¤ Streaming Conversation Active - Speak to Synapse! ðŸ”Š</div>
                        )}
                        
                        {/* Test Image Generation Button (for debugging) */}
                        {isDemoUser && (
                            <div className="mt-2 text-center">
                                <button
                                    onClick={(e) => {
                                        e.stopPropagation();
                                        setInputMessage("Generate an image of a cute robot");
                                        setTimeout(() => handleSendText(), 100);
                                    }}
                                    className="px-3 py-1 bg-purple-500/20 border border-purple-400/30 rounded-lg text-purple-300 text-xs hover:bg-purple-500/30 transition-all"
                                >
                                    ðŸ§ª Test Image Gen
                                </button>
                            </div>
                        )}
                    </div>
                </div>
            </aside>


        </>
    );
};

export default AIAssistant;